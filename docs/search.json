[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Modeling and Computation for Educational Scientists",
    "section": "",
    "text": "Front Matter\nI am putting this out there primarily as a resource for my own students to help them through the installation and provide written instruction for the computational tools we use in the courses EPsy 8251. As such, I will initially focus on R and RStudio, but I will probably add tools (e.g., reference managers, github, make) over time as I have time. (Note: If you want to contribute to this, create a Pull Request or send me an email.) Also, feel free to offer criticism, suggestion, and feedback. You can either open an issue on the book‚Äôs github page or send me an email directly."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Statistical Modeling and Computation for Educational Scientists",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nMany thanks to all the students in my courses who have been through previous iterations of this material. Your feedback has been invaluable, and you are the world‚Äôs greatest copyeditors. In particular, I would like to thank the following students who have gone above and beyond in the feedback they have provided: Jonathan Brown, Pablo Vivas Corrales, Amaniel Mrutu, Corissa Rohloff, and Mireya Smith."
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "Statistical Modeling and Computation for Educational Scientists",
    "section": "Colophon",
    "text": "Colophon\nArtwork by @allison_horst\nIcon and note ideas and prototypes by Desir√©e De Leon.\nThe book is typeset using Crimson Text for the body font, Raleway for the headings and Sue Ellen Francisco for the title. The color palette was generated using coolors.co.\nStatistical Computing\n\nLaptop icon made by Tomas Knop from www.flaticon.com\nDirectory icon made by Darius Dan from www.flaticon.com\nBrain icon made by Aranagraphics from www.flaticon.com\nInternet icon made by Freepik from www.flaticon.com"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Statistical Modeling and Computation for Educational Scientists",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "01-00-statistical-computation.html#footnotes",
    "href": "01-00-statistical-computation.html#footnotes",
    "title": "Introduction to Statistical Computation",
    "section": "",
    "text": "Specifically, RStudio is branded as an ‚Äúintegrated development environment (IDE) [that] includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.‚Äù‚Ü©Ô∏é"
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#installing-r",
    "href": "01-01-r-and-rstudio-installation.html#installing-r",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.1 Installing R",
    "text": "1.1 Installing R\nTo install R, navigate your web browser to:\n\nhttps://www.r-project.org/\n\nThen,\n\nClick the CRAN link under Download on the left-hand side of the page.\nSelect a mirror site. These should all be the same, but I tend to choose the Iowa State University link under USA.1\nIn the Download and Install R box, choose the binary that matches the operating system (OS) for your computer.\n\nThis is where the installation directions diverge depending on your OS.\nMac Instructions\nSo long as you are running MacOS 10.13 or higher just click the first link for the PKG, which will download the installer for the most current version of R (4.1.1 as of August 16, 2021). Once the download completes, open the installer and follow the directions to install R on your computer.\nIf you are running an older version of MacOS, you will have to install an older version of R. You can find these links under the Binaries for legacy OS X systems heading further down the install page. Click the appropriate PKG link for R your version of MacOS. Once the download completes, open the installer and follow the directions to install R on your computer.\nIf you are unsure which version of the MacOS is running on your computer, select About this Mac from the Apple menu in your toolbar.\nWindows Instructions\nClick the link that says Install R for the first time (or click base; they go to the same place). Then click the Download R 4.1.1 for Windows link, which will download the installer for the most current version of R (4.0.2 as of July 24, 2020). Once the download completes, open the installer and follow the directions to install R on your computer.\nLinux Instructions\nIf you are running Linux, you should know how to install things on your computer. üòÉ"
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#installing-rstudio-desktop",
    "href": "01-01-r-and-rstudio-installation.html#installing-rstudio-desktop",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.2 Installing RStudio Desktop",
    "text": "1.2 Installing RStudio Desktop\nAfter you have installed R, you next need to install RStudio Desktop. To do this, navigate your web browser to:\n\nhttps://rstudio.com/products/rstudio/download/\n\nThen,\n\nSelect the blue Download button under the free, open-source version of RStudio Desktop.\nSelect the installer associated with your computer‚Äôs OS.\nOnce the download completes, open the installer and follow the directions to install RStudio Desktop on your computer."
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#checking-that-things-worked",
    "href": "01-01-r-and-rstudio-installation.html#checking-that-things-worked",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.3 Checking that Things Worked",
    "text": "1.3 Checking that Things Worked\nFrom your Applications or Programs folder, open RStudio. If you have successfully downloaded both programs, this should open the application and you should see a message indicating that you are using ‚ÄúR version 4.1.1‚Äù (or whichever version of R you installed) in the console pane.\n\n\n\n\n\nOnce you open RStudio, you should see a message indicating that you are using R version 4.1.1 (or whichever version of R you installed) in the console pane. Here the console pane is on the left-side, but it may be in a different location for you. Your RStudio may also have a white background rather than the black background seen here."
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#customizing-rstudio",
    "href": "01-01-r-and-rstudio-installation.html#customizing-rstudio",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.4 Customizing RStudio",
    "text": "1.4 Customizing RStudio\nWhile the information in this section is not crucial for making things work, it is useful to get RStudio looking good and setting some default settings. Open the Tools &gt; Options menu (Windows) or RStudio &gt; Preferences (Mac).\n\n\n\n\n\nThe RStudio options/preferences menu has many settings to customize RStudio.\n\n\n\n\n\nIn the General &gt; Basic settings, change the option on Save workspace to .Rdata on exit to be ‚ÄúNever‚Äù. Click the ‚ÄúApply‚Äù button.\nIn the Appearance settings, customize the look of RStudio to something aesthetically appealing to you. When you are finished, click the ‚ÄúApply‚Äù button.\nThere are also options you can set in the Accessibility settings if you use a screen reader. If you change anything, don‚Äôt forget to click the ‚ÄúApply‚Äù button.\n\nWhen you are finished customizing RStudio, click the ‚ÄúOK‚Äù button."
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#install-rtoolscommand-line-tools",
    "href": "01-01-r-and-rstudio-installation.html#install-rtoolscommand-line-tools",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "1.5 Install Rtools/Command Line Tools",
    "text": "1.5 Install Rtools/Command Line Tools\nYou may need to install some additional functionality to your system in order to get certain packages to install or load properly. On a Windows machine, you might need to install Rtools. Mac users might need to add the Command Line Tools. These tools also allow you to write and compile your own R packages. RStudio has well written instructions for adding these tools at: https://support.rstudio.com/hc/en-us/articles/200486498-Package-Development-Prerequisites."
  },
  {
    "objectID": "01-01-r-and-rstudio-installation.html#footnotes",
    "href": "01-01-r-and-rstudio-installation.html#footnotes",
    "title": "1¬† R and RStudio: Installation and Setup",
    "section": "",
    "text": "When internet used to be dial-up (i.e., super slow), you wanted to choose a mirror site that was closest in proximity to your location as it sped up the download. This is less of a concern now that internet download speeds are much faster.‚Ü©Ô∏é"
  },
  {
    "objectID": "01-02-getting-started-with-r.html#computing-in-the-console",
    "href": "01-02-getting-started-with-r.html#computing-in-the-console",
    "title": "2¬† Getting Started with R",
    "section": "2.1 Computing in the Console",
    "text": "2.1 Computing in the Console\nThere are a several ways to interact with R within RStudio. One of those methods is to issue commands in the RStudio console pane. Computing in the console is similar to working with a calculator: you enter syntax and R runs the syntax. The &gt; symbol in the console pane is called the ‚ÄúR prompt‚Äù and is prompting you to enter syntax. After you enter any syntax, you hit the  or  key to execute the syntax.\n\n\n\n\n\nThe console pane in RStudio is one way to compute with R. Syntax is entered at the R prompt and executed by hitting the  or  key.\n\n\n\n\nTo get started, we will have R carry out some arithmetic. At the prompt enter each of the following lines of syntax. After each line, hit the &lt;return&gt; or &lt;enter&gt; key.\n\n# Addition\n2 + 3\n\n# Subtraction\n6 - 10\n\n# Multiplication\n4 * 5\n\n# Division\n23 / 2\n\n# Exponents\n10 ^ 3\n\nExecuting each line of syntax returns the results of the computation in the console pane. The result, in R parlance, is referred to as a ‚Äúreturned value‚Äù. After executing the computation, the R prompt reappears and you can issue a new line of syntax.\nContinuation Prompt: At some point in your computational career, you will likely encounter the continuation prompt. Symbolized by +, this prompt appears instead of the R prompt to indicate that you did not complete the syntax you entered prior to hitting the  key. For example, suppose while entering the division syntax from above you got excited and hit the &lt;enter&gt; key after inputting the slash; before inputting the 2. You would see the continuation prompt:\n&gt; 23 /\n+ \nThe continuation prompt tells you that the syntax you started to enter is still active. If you now enter 2 and hit the &lt;enter&gt; key, the syntax will be executed as if you had not inadvertently hit &lt;enter&gt; in the middle of the computation. If you are in the middle of a more complex piece of syntax, you could also hit the &lt;esc&gt; key until the R prompt is re-shown. Then you can start the computation over.\nSpace in Syntax: For the computation in R, space is irrelevant. For example, each of the following syntactical statements is equivalent:\n\n4 * 5\n4*5\n4    *            5\n\nThat being said, well-written code includes space! Space in code makes it easier to read and debug, in the same way that including space in written prose helps us read and parse words, sentences, and paragraphs."
  },
  {
    "objectID": "01-02-getting-started-with-r.html#functions-the-workhorse-of-r",
    "href": "01-02-getting-started-with-r.html#functions-the-workhorse-of-r",
    "title": "2¬† Getting Started with R",
    "section": "2.2 Functions: The Workhorse of R",
    "text": "2.2 Functions: The Workhorse of R\nAlmost all commands in R are built around the use of a function. Functions carry out operations on their inputs (called arguments) and produce an output (called a returned value).\n\n\n\n\n\nLEFT: Arguments are inputted into a function which returns an output. RIGHT: The value 25 is inputted into the square root function which returns the value of 5.\n\n\n\n\nThe syntax for using most functions in R follows a simple structure: The name of the function is followed by a pair of parentheses. Argument values (inputs) are specified inside the parentheses. In general,\n\nfunction_name(argument)\n\nBelow are several example of this structure for some common mathematical functions.\n\n#Square root\nsqrt(100)\n\n[1] 10\n\n#Absolute value\nabs(-23)\n\n[1] 23\n\n#Factorial\nfactorial(5)\n\n[1] 120\n\n\nFunctions can take multiple arguments (inputs). If there is more than one argument, the arguments are always separated by a comma.\n\nfunction_name(argument_1, argument_2, ...)\n\nFor example, the seq() function creates a sequence of values that have a particular start and end value.\n\nseq(1, 5)\n\n[1] 1 2 3 4 5\n\n\nNote that the order of the arguments matters! Reversing the order of the arguments inputted to the seq() function returns different output.\n\nseq(5, 1)\n\n[1] 5 4 3 2 1\n\n\nEach of the arguments actually has a name, and if those names are used to assign the values we are inputting to the function, then the order of the arguments is irrelevant.\n\n# Named arguments\nseq(from = 1, to = 5)\n\n[1] 1 2 3 4 5\n\n# Order no longer matters\nseq(to = 5, from = 1)\n\n[1] 1 2 3 4 5\n\n\nIt is a good habit to use named arguments when you are writing your syntax. It makes the code more readable and easier to adapt if you come back to it later. Sometimes when the initial argument of a function is well-established, that first argument is left unnamed, but all other arguments used are named. A good example of this is the use of the lm() function in which the initial formula= argument is typically unnamed, but other arguments (e.g., data=) are named."
  },
  {
    "objectID": "01-02-getting-started-with-r.html#connecting-computations",
    "href": "01-02-getting-started-with-r.html#connecting-computations",
    "title": "2¬† Getting Started with R",
    "section": "2.3 Connecting Computations",
    "text": "2.3 Connecting Computations\nOne powerful aspect of computing is that the output from a function can be used as input into another function. This is akin to calculations you may have encountered in mathematics course like algebra. For example,\n\\[\n\\sqrt{\\log(100)}\n\\]\nIn carrying out this calculation, you would first compute the logarithm of 100 and then take the square root of that result. Using function notation, your algebra teacher might have written\n\\[\ng(~f(x)~)\n\\]\nwhere \\(f(x)\\) is the logarithm function and \\(g(x)\\) is the square root function. Regardless of how we express this,the idea is that the output of the logarithm function is used as input to the square root function. Using R, there are two primary ways to connect computations in this manner: chaining and assignment.\n\n\n2.3.1 Chaining\nChaining is a direct reflection of the functional notation \\(g(~f(x)~)\\) in that you embed one computation directly in another. For example, to find the absolute value of all the integers between \\(-5\\) and \\(-1\\), we can chain the abs() function and the seq() functions:\n\nabs( seq(from = -5, to = -1) )\n\n[1] 5 4 3 2 1\n\n\nNote here that the output/returned value is not a single value, but five values. The absolute value function was applied to each of the values in the sequence. Compare this to the result from the following chained computations:\n\nmean( seq(from = -5, to = -1) )\n\n[1] -3\n\n\nHere the mean function applied to the sequence of values returns a single value as output. When a function is applied to a set of multiple values (i.e., a vector), some functions will apply their computations separately on each element/value (element-wise computation returns multiple values), while others will apply the computation to the set of elements as a whole (vector-wise computation returns a single value). Most of the time it will be clear from the function name or description whether the output returns a single value or multiple values.\nWe can chain as many computations together as we would like. For example here we find the square root of the absolute value of all the integers between \\(-5\\) and \\(-1\\).\n\nsqrt( abs( seq(from = -5, to = -1) ) )\n\n[1] 2.236068 2.000000 1.732051 1.414214 1.000000\n\n\n\n\n\n2.3.2 Assignment\nWe can also connect computations through assignment. With assignment, we store the output of a computation by assigning it to a named object. We can then use that named object in another computation. For example, here we first store the integers between \\(-5\\) and \\(-1\\) in an object called chili. Then we find the absolute value of each value by using chili as the argument in the abs() function.\n\n# Assign the sequence to the object chili\nchili = seq(from = -5, to = -1) \n\n# Compute the absolute values\nabs(chili)\n\n[1] 5 4 3 2 1\n\n\nTo view the contents of an object, just print the name of the object. Below, after creating the object sadie, we view its contents.\n\n# Assign values to sadie\nsadie = rep(3, times = 5) \n\n# View contents of sadie\nsadie\n\n[1] 3 3 3 3 3\n\n\nPretty much any name can be used when you create an object, with some caveats: object names cannot begin with a digit nor include hyphens or spaces. Although they are legal object names, chili and sadie are not particularly good object names. Better object names would describe the contents of the object.\nIn my own workflow, I tend to use all lowercase letters in my object names and I use underscores for word breaks. For example,\n\nses, gpa, occupation\nact_math, mothers_educ\n\n\n\n\n2.3.3 Objects in the R Working Environment\nWe can continue to use the objects you created (e.g., chili and sadie) in our computations, so long as the objects remain in our R working environment.\n\n# Sum the pairwise elements of the two objects\nchili + sadie\n\n[1] -2 -1  0  1  2\n\n# Sum all the elements in chili\nsum(chili)\n\n[1] -15\n\n# Product of all the elements in sadie\nprod(sadie)\n\n[1] 243\n\n\nIn RStudio you can see which objects are in your working environment by examining the Environment pane.\n\n\n\n\n\nThe enivironment pane shows the objects in the R working environment. It also displays the object‚Äôs class and gives a preview of the the contents.\n\n\n\n\nNot only does this pane indicate the name of the objects in the working environment, but is also displays each object‚Äôs class. In this case we can tell that chili is an integer vector and sadie is a numeric vector.1 Moreover, we are told that each vector includes five elements, shown in the environment pane as [1:5]. Lastly, we are given a preview of each object‚Äôs contents. Since these vectors only contain five elements, we see all the values in the preview.\nYou can also use syntax to obtain a list of objects that are in your working environment using the ls() function with no arguments.\n\n# List the objects in working environment\nls()\n\n[1] \"chili\"           \"has_annotations\" \"sadie\"          \n\n\nWhen an object name is re-used, the previous value of the object is lost.\n\n# Assign the values 1 to 10 in chili\nchili = seq(from = 1, to = 10)\n\nAfter assigning new values to chili, we can see the information in the environment pane has been updated to reflect the new contents of the object.\n\n\n\n\n\nThe object chili although still an integer vector, now includes 10 elements.\n\n\n\n\nAny computations carried out with chili will use the new object.\n\n# Sum all the elements in chili\nsum(chili)\n\n[1] 55\n\n\nIf you want the previous version of chili you need to re-create the object. If you close your R session all of the objects you created will be lost."
  },
  {
    "objectID": "01-02-getting-started-with-r.html#script-files-recording-your-syntax",
    "href": "01-02-getting-started-with-r.html#script-files-recording-your-syntax",
    "title": "2¬† Getting Started with R",
    "section": "2.4 Script Files: Recording Your Syntax",
    "text": "2.4 Script Files: Recording Your Syntax\nIt is important to be able to record the syntax you use. This acts as a way of ‚Äúsaving‚Äù your work, and it also acts as a record of the analysis for your collaborators in the spirit of reproducible work. One way to record the R syntax you use is to employ a script file. You can create a new script file by selecting File &gt; New File &gt; R Script from the RStudio menu bar. You can also obtain a new R script by clicking the New File icon (document with the plus-sign) on the tool bar and selecting R Script.\n\n\n\n\n\nLEFT: Create a new script file using RStudio‚Äôs File menu. RIGHT: Create a new script file by clicking on the New File icon in the toolbar.\n\n\n\n\nScript files should only include your R syntax and comments. Script files should NOT include:\n\nprompts (&gt;)\noutput\n\nComments, which are human-readable annotations or explanations of the syntax, are written using the hashtag (#). These can be placed on their own line in the script file, or can be placed at the end of a line with syntax. The comment continues until you hit the  key. Comments help other people understand your syntax and also act as a reminder for the future you of what your code actually does. Get is the habit of including comments in your syntax. Not only is it good coding practice, but it also will help you become familiar with and learn the R syntax as you describe the purpose of the syntax you are writing.\n\n\n\n\n\nExample script file with comments.\n\n\n\n\nScript files can be saved and opened the same as any other document. So, no more worrying about losing your work or objects that you created when you close your R sesssion. Just open your saved script file, highlight the parts you want to re-run, and click the Run button!\n\n\n2.4.1 Executing Syntax from a Script File\nNot only does the script file record your syntax, but it can also act as the vehicle from which you run your R syntax. Syntax in the script file can be executed by highlighting it and pressing the Run button in the toolbar. You can run one line at a time, or highlight multiple lines and execute all of them sequentially.\n\n\n\n\n\nTo execute syntax from the script file, highlight the syntax you want to run and then click the Run button in the toolbar.\n\n\n\n\nWriting syntax directly in the script file and running it is a groovy workflow for using R. Writing syntax directly in the script file also saves you from having to copy-and-paste syntax you want to save from the console. In my own work, I use this workflow almost daily."
  },
  {
    "objectID": "01-02-getting-started-with-r.html#installing-and-loading-r-packages",
    "href": "01-02-getting-started-with-r.html#installing-and-loading-r-packages",
    "title": "2¬† Getting Started with R",
    "section": "2.5 Installing and Loading R Packages",
    "text": "2.5 Installing and Loading R Packages\nEvery R function is housed in a package. To use the functions in a particular package, the package needs to be (1) installed, and (2) loaded into memory.\n\n\n\n\n\nPackages need to be installed and loaded.\n\n\n\n\nYou can see the packages (and which version of each package) are installed by examining the Packages tab in RStudio. Every package listed there has been installed. You will also be able to see the version number of the package that is installed. Some of those packages may be checked. These are the packages that are also loaded into memory.\n\n\n\n\n\nThe packages tab shows which packages are installed. The list of packages you have installed will likely be different. Checked packages are loaded into memory. In the packages seen here, only the base package is loaded into memory.\n\n\n\n\nTwenty-nine packages were included when you installed R on your computer. When you start an R session by opening RStudio, some of those packages are also loaded into memory.\n\n\n\n\n\nThe 29 packages installed as part of R. The base, datasets, graphics, grDevices, methods, stats, and utils packages are loaded into memory when you start an R session. The other 22 packages are installed but not loaded into memory.\n\n\n\n\n\n\n2.5.1 Loading Packages into Memory\nTo load a package that is installed, you use the library() function and include the name of the package you want to load in as the sole argument. For example, to load the {splines} package, use the following syntax:\n\n# Load splines package\nlibrary(splines)\n\nSome packages requires other packages (dependencies) to work. For example, the {lme4} package is dependent on the {Matrix} package. When you load packages that have dependencies, R will also load the dependencies (assuming you have them installed). When it does this, a message will be printed after you execute library(). For example, here is what happens when we load the {lme4} package.\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nOnce the package is loaded, all of the functions, data sets, etc. in that package are available to you. Packages will need to be loaded every time you launch a new R session.\n\n\n\n2.5.1.1 Masked Objects\nSometimes you will get a message about objects being masked. This is not a problem. It is just informative and means that the package you just loaded has a function that has the exact same name as a previously loaded package. If you use that particular function, the most recently loaded package‚Äôs version of the function will be used. For example, after loading the {dplyr} library the following message is printed:\nAttaching package: ‚Äòdplyr‚Äô\n\nThe following objects are masked from ‚Äòpackage:stats‚Äô:\n\n    filter, lag\n\nThe following objects are masked from ‚Äòpackage:base‚Äô:\n\n    intersect, setdiff, setequal, union\nThis tells me that the {dplyr} package includes six functions that have the same name as functions included in other packages that have already been loaded into memory (two from the {stats} package and four from the {base} package.) Since {dplyr} was the most recently loaded package, if you were to use the filter() function, R would use {dplyr}‚Äôs filter() function rather than that from the {stats} package2.\n\n\n\n2.5.1.2 Error Loading a Package\nOnce in a while, when loading a package, you may get an error. Don‚Äôt panic. There are two errors that are common. The first error you may get indicates that the package did not get installed. For example, if the {ggplot2} package was not installed, trying to use the library() function to load that package would result in an error.\n\nlibrary(ggplot2)\n\nError in library(ggplot2) : there is no package called ‚Äòggplot2‚Äô\nSimply install the package and then re-try loading it.\nThe second error that you might run across when trying to load a package occurs when the installation did not include the package dependencies. For example,\n\nlibrary(odbc)\n\nError: package or namespace load failed for ‚Äòodbc‚Äô in loadNamespace(j &lt;- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):\nthere is no package called ‚ÄòRcpp‚Äô\nAgain, don‚Äôt panic! Here the error message is saying that the {Rcpp} package is a dependency and it is not installed. To fix this, install the {Rcpp} package and then try again. (You may need to open a new R session first.) You may need to install more than one dependency, so just keep installing what is missing until it works.\n\n\n\n\n2.5.2 Adding Functionality: Installing Packages\nYou can also install other packages onto your R system. R, in fact, has a repository called CRAN3, that includes 16,000 different packages (as of July 2020). The easiest way to install a package from CRAN onto your computer is to use the Install button in the `Packages tab of RStudio.\nThis will open a pop-up window where you can type the CRAN package you want to install in a text box. Ensure that the ‚ÄúInstall dependencies‚Äù box is checked (this will also install any package dependencies), and then click ‚ÄúInstall‚Äù.\n\n\n\n\n\nPop-up window to install packages. Here we are installing the dplyr package. Note that the ‚ÄòInstall dependencies‚Äô box is checked.\n\n\n\n\nYou may be prompted to choose a nearest mirror. If so, choose a mirror location. If you are successful in installing the package, you will get a message like the following:\nInstalling package into ‚Äò/Users/zief0002/Library/R/4.0/library‚Äô\n(as ‚Äòlib‚Äô is unspecified)\ntrying URL 'https://cran.rstudio.com/bin/macosx/contrib/4.0/dplyr_1.0.0.tgz'\nContent type 'application/x-gzip' length 1209135 bytes (1.2 MB)\n==================================================\ndownloaded 1.2 MB\n\n\nThe downloaded binary packages are in\n    /var/folders/s3/sqlc9xw92w54166w86dgkd000000gr/T//Rtmps80Uf8/downloaded_packages\nThe message you get on Windows may be slightly different, but the key is that there is not an error. Furthermore, you should immediately be able to load the package using the library() function.\nAn equivalent manner of installing a package via syntax is to use the install.packages() function. For example, to install the {dplyr} package we could have used the following syntax:\n\ninstall.packages(\"dplyr\", dependencies = TRUE)\n\nNote that the name of the package is included in quotation marks (it is a character string). The argument dependencies=TRUE installs all package dependencies, similar to checking the ‚ÄúInstall dependencies‚Äù box in the pop-up menu.\n\n\n2.5.2.1 Installing Packages from GitHub\nCRAN is not the only place to get R packages. Many developers add packages to a website called GitHub. Packages hosted on GitHub can be installed using the install_github() function from the {remotes} package.\nFirst, you will need to install the {remotes} package from CRAN and then load it using the library() function. Then, you can use the install_github() function to actually install the package. This function is provided a character string that specifies the user name and GitHub repository for the package, separated by a slash. You can find this in the part of the URL that follows ‚Äúhttps://github.com/‚Äù in your web browser. For example, the URL for the {educate} package is: ‚Äúhttps://github.com/zief0002/educate‚Äù, so to install this we would use:\n\n# Load remotes package\nlibrary(remotes)\n\n# Install dplyr from GitHub\ninstall_github(\"zief0002/educate\")\n\nThe message I get when installing this is\nInstalling package into ‚Äò/Users/zief0002/Library/R/4.0/library‚Äô\n(as ‚Äòlib‚Äô is unspecified)\n* installing *source* package ‚Äòeducate‚Äô ...\n** using staged installation\n** R\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n*** copying figures\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (educate)\nThe ‚ÄúDONE‚Äù message typically signified successful installation. Note that you may be prompted to update some packages. If you get this message, choose the option to update ‚ÄúALL‚Äù packages. As with packages installed from CRAN, if things worked you should be able to load the package you just installed using the library() function without any errors.\n\n\n\n\n2.5.3 Install Rtools/Command Line Tools\nYou may need to install some additional functionality to your system in order to get certain packages to install or load properly. On a Windows machine, you might need to install Rtools. Mac users might need to add the Command Line Tools. These tools also allow you to write and compile your own R packages. RStudio has well written instructions for adding these tools at: https://support.rstudio.com/hc/en-us/articles/200486498-Package-Development-Prerequisites."
  },
  {
    "objectID": "01-02-getting-started-with-r.html#footnotes",
    "href": "01-02-getting-started-with-r.html#footnotes",
    "title": "2¬† Getting Started with R",
    "section": "",
    "text": "The difference between the two classes is technical and related to how R internally stores the information in the vector.‚Ü©Ô∏é\nIf you really wanted to use the filter() function from the {stats} package you could specify this in the syntax using the :: operator, stats::filter(). This operator also allows you to use a function without loading the package with the library() function.‚Ü©Ô∏é\n‚ÄúCRAN is a network of ftp and web servers around the world that store identical, up-to-date, versions of code and documentation for R.‚Äù‚Ü©Ô∏é"
  },
  {
    "objectID": "01-03-data-structures-in-r.html#vectors",
    "href": "01-03-data-structures-in-r.html#vectors",
    "title": "3¬† Data Structures in R",
    "section": "3.1 Vectors",
    "text": "3.1 Vectors\nVectors (the single-column bookcases in our metaphor) are perhaps the most common data structure you will encounter in R. In fact, even the data frame is composed of vectors; each column is a vector. There are many ways to create a vector in R, in fact you have already been introduced to a couple of them: seq() and rep(). These are useful to create sequences of values and vectors with repeated values, respectively. But what if you wanted to create the vector of the Spice Girls‚Äô ages when the band was formed in 1994 (the values in the second column in the picture above)?\nTo create a vector of these ages, we can use the c() function to input each of the five ages. Within this function, each age is separated by a comma‚Äîeach input is a separate argument to the c() function. We will also assign this to an object called age.\n\n# Create age vector\nage = c(19, 20, 18, 22, 20)\n\n# View vector\nage\n\n[1] 19 20 18 22 20\n\n\nNote that once we assign create age it shows up in our global environment. In the technical language of R, each age is an element of the vector. All of the elements in the age vector are numeric values. This is the vector‚Äôs type.3 Lastly, there are five elements in the vector.\nOnce you have created a numeric vector, you can compute on it. For example in the syntax below we compute the mean age, the standard deviation of the ages, and count the elements in the vector.\n\n# Compute mean\nmean(age)\n\n[1] 19.8\n\n# Compute standard deviation\nsd(age)\n\n[1] 1.48324\n\n# Count elements\nlength(age)\n\n[1] 5\n\n\n\n\n3.1.1 Logical Vectors\nAnother common vector type you will encounter is the logical vector. Each element in a logical vector is either TRUE or FALSE (all uppercase letters). You could use the c() function to create a logical vector. For example, to create the original_member vector we could use the following syntax:\n\n# Create logical vector\noriginal_member = c(TRUE, TRUE, FALSE, TRUE, FALSE)\n\n# View vector\noriginal_member\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE\n\n\nIt is more common to create logical vectors through computation using logical operators. For example we might ask which elements of the age object are greater than 20.\n\n# Which elements of age &gt; 20\nage &gt; 20\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\nThe result of using the logical operator &gt; is a logical vector. There are several logical operators in addition to &gt;:\n\n# greater than or equal to 20\nage &gt;= 20\n\n[1] FALSE  TRUE FALSE  TRUE  TRUE\n\n# less than 20\nage &lt; 20\n\n[1]  TRUE FALSE  TRUE FALSE FALSE\n\n# less than or equal to 20\nage &lt;= 20\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE\n\n# equal to 20\nage == 20\n\n[1] FALSE  TRUE FALSE FALSE  TRUE\n\n# not equal to 20\nage != 20\n\n[1]  TRUE FALSE  TRUE  TRUE FALSE\n\n\nNote that the logical operator for ‚Äúequal to‚Äù is two equals signs. This is because = (one equal sign) is what we use for assignment . If you wrote age=20 you would be assigning the value 20 to age, not asking whether the elements in age are equal to 20!\nLogical elements have numeric values associated with them, namely,\n\nFALSE = 0; and\nTRUE = 1.\n\nThis means we can apply computations to a logical vector. For example, we could count the number of Spice Girls that were original members by summing the logical values in the original_members object. (Since all FALSE values are 0, this amounts to counting the number of TRUE values.)\n\n# Count original members\nsum(original_member)\n\n[1] 3\n\n\nWe could also count the number of Spice Girls who are under the age of 20.\n\n# Count members with age &lt; 20\nsum(age &lt; 20)\n\n[1] 2\n\n\n\n\n\n3.1.2 Character Vectors\nA third type of vector you will work with is a character vector. Character vectors (a.k.a., strings, literals) are vectors in which each element is a string of characters delimited by quotation marks. For example, the Spice names column is a character vector. We can again create this vector using the c() function.\n\n# Create character vector\nspice = c(\"Scary\", \"Sporty\", \"Baby\", \"Ginger\", \"Posh\")\n\n# View vector\nspice\n\n[1] \"Scary\"  \"Sporty\" \"Baby\"   \"Ginger\" \"Posh\"  \n\n\nMany computations that worked on numeric vectors do not work on character vectors. These will often return an error or unexpected result. In the syntax below, for example, we are told that the mean() function expects a numeric or logical vector, and since what we used was not either of those, the result returned was NA.\n\n# Find mean name\nmean(spice)\n\nWarning in mean.default(spice): argument is not numeric or logical: returning\nNA\n\n\n[1] NA\n\n\nSome computations work the same way.\n\n# Count the number of elements\nlength(spice)\n\n[1] 5"
  },
  {
    "objectID": "01-03-data-structures-in-r.html#data-frames",
    "href": "01-03-data-structures-in-r.html#data-frames",
    "title": "3¬† Data Structures in R",
    "section": "3.2 Data Frames",
    "text": "3.2 Data Frames\nData frames (the multi-column bookcases in our metaphor) are a more complex data structures than vectors. There are again, multiple ways to create a data frame in R. We will examine two methods for creating a data frame: using the data.frame() function and importing data from a spreadsheet or CSV file.\nTo create a data frame from scratch, using R, we can use the data.frame() function. Each argument to this function is a named vector that will correspond to a column within the data frame, and each argument (vector) is separated by commas. For example, to create the Spice Girls data frame from our example, we could use the following syntax:\n\n# Create data frame\nspice_girls = data.frame(\n  spice = c(\"Scary\", \"Sporty\", \"Baby\", \"Ginger\", \"Posh\"),\n  age = c(19, 20, 18, 22, 20),\n  original_member = c(TRUE, TRUE, FALSE, TRUE, FALSE),\n  solo_nominations = c(4, 26, 14, 13, 12),\n  real_name = c(\"Mel B\", \"Mel C\", \"Emma\", \"Geri\", \"Victoria\")\n)\n\n# View data frame\nspice_girls\n\n\n\n  \n\n\n\nNote that we also assigned the data frame to an object called spice_girls so we can compute on it. You will learn how to compute on data frames in the chapters Data Wrangling with dplyr and Plotting with ggplot2."
  },
  {
    "objectID": "01-03-data-structures-in-r.html#importing-data-from-a-csv-file",
    "href": "01-03-data-structures-in-r.html#importing-data-from-a-csv-file",
    "title": "3¬† Data Structures in R",
    "section": "3.3 Importing Data From a CSV File",
    "text": "3.3 Importing Data From a CSV File\nIn professional practice, you will often enter data into a spreadsheet and rather than typing it into R. When you save this work, many spreadsheet programs use a proprietary format for saving the information (e.g., Excel saves as a XLSX file; Google Sheets saves as a GSHEET file). These often include extraneous information (e.g., formatting) that is irrelevant to the raw data. While R includes libraries and functions that can import data in the XLSX and GSHEETS formats, it is generally easier to save or export your data to a CSV (comma separated value) file from within your spreadsheet program prior to importing it into R.\n\n\n\n\n\n\nHere are some tips for entering data into a spreadsheet:\n\nThe first row should be the variable names. Do not use spaces in variable names.\nCharacter strings should be entered without quotation marks in a spreadsheet.\nIf you have missing data, leave the cell blank.\n\nFor more tips on entering data, see Broman & Woo (2018).\n\nOnce your data are saved as a CSV file, it can be easily imported into R. To do so,\n\nClick the Import Dataset button under the Environment tab in RStudio and choose ‚ÄúFrom Text (readr)‚Äù.\nIf the CSV file is a file stored on your computer, click the Browse button and navigate to where you saved your CSV file, select the file, and click ‚ÄúOpen‚Äù. If the CSV file is hosted on the web, type the URL into the ‚ÄúFile/URL‚Äù text box and click ‚ÄúUpdate‚Äù.\n\n\n\n3.3.1 Importing the Spice Girls Data\nThe file spice-girls.csv is accessible at https://raw.githubusercontent.com/zief0002/toolkit/master/data/spice_girls.csv.\n\nCopy and paste that URL into the ‚ÄúFile/URL‚Äù text box.\nClick the ‚ÄúUpdate‚Äù button.\n\nClicking ‚ÄúUpdate‚Äù will open a preview of your data. Check to be sure the variable names are correct and that the data looks like what you entered into your spreadsheet.\n\nChange the text in the name box to correspond to the object name you want to use in R.\nFinally, click the Import button to import your data.\n\nAfter importing the data you should see the object in your global environment.\n\n\n\n\n\n\n\n\n3.3.2 Importing Data Using a Script File\nEven though you used the Import button‚Äîa point-and-click feature in RStudio‚Äîto import the data, behind the scenes, syntax was generated that was actually used to import the data into R. When we selected ‚ÄúFrom Text (readr)‚Äù, the read_csv() function from the {readr} package was used to import the data. You can see the syntax generated in the Code Preview window after you selected your CSV file.\n\n\n\n\n\nIn the first line of syntax, the {readr} package is loaded using the library() function. The data is imported in the second line of syntax and assigned to an object, in this case spice_girls. The read_csv() function includes an unnamed argument providing the URL for the CSV file.4 The View() function in the third line of syntax simply opens the spice_girls object in a view tab in RStudio.\nIt is a good idea to copy the first two lines of syntax from the Code Preview window into your script file. It will be faster to import the data in the future by running it from a script file rather than trying to reproduce all the steps to import your data. The third line of syntax, using View(), is not essential to importing your data..\n\nSince there are better ways to actually ‚Äúsee‚Äù the contents of the data object (e.g., print()), you should not include the View() syntax line in your script file.\n\nBelow are the two lines I would include in the script file. I would also comment them.\n\n# Load readr library\nlibrary(readr)\n\n# Import data\nspice_girls &lt;- read_csv(\"https://raw.githubusercontent.com/zief0002/toolkit/master/data/spice_girls.csv\")\n\n\nThe syntax &lt;- is another way to write the assignment operator. You can use either = or &lt;- for the assignment operator. Whichever you choose, be consistent!"
  },
  {
    "objectID": "01-03-data-structures-in-r.html#validity-check-on-imported-data",
    "href": "01-03-data-structures-in-r.html#validity-check-on-imported-data",
    "title": "3¬† Data Structures in R",
    "section": "3.4 Validity Check on Imported Data",
    "text": "3.4 Validity Check on Imported Data\nOnce you import data, you should always perform a validity check to ensure that the entire dataset was imported and that things look OK. There are several functions that are useful for this examination. Three that I use regularly are print(), glimpse() and summary().\nThe print() function gives us a quick look at the data.\n\n# View data\nprint(spice_girls)\n\n# A tibble: 5 √ó 5\n  spice_name   age original_member solo_nominations real_name\n  &lt;chr&gt;      &lt;dbl&gt; &lt;lgl&gt;                      &lt;dbl&gt; &lt;chr&gt;    \n1 Scary         19 TRUE                           4 Mel B    \n2 Sporty        20 TRUE                          26 Mel C    \n3 Baby          18 FALSE                         14 Emma     \n4 Ginger        22 TRUE                          13 Geri     \n5 Posh          20 FALSE                         12 Victoria \n\n\nNote that from this output we can see that the read_csv() function actually imports the data as a tibble. Tibbles are essentially the same data structure as data frames. The only difference is that when you use print() (and some other functions) to examine the data object, what is printed to the screen is slightly different.5 For tibbles,\n\nThe number of rows and columns is displayed;\nThe first 10 rows are shown;\nOnly the columns that fit on screen are printed; and\nEach column type is reported\n\nHere the size of the data object is 5 x 5, which indicates that there are five rows (first value) and five columns (second value). We are also informed which columns are numeric, which are logical, and which are character.6\nThe summary() function computes summary statistics for each column in the data object. Different measures are computed depending on the column type. For character columns, only the length of the column is computed. The count of TRUE and FALSE values are computed for logical columns, and several measures are computed for numeric columns.\n\n# Compute summary measures for each column\nsummary(spice_girls)\n\n  spice_name             age       original_member solo_nominations\n Length:5           Min.   :18.0   Mode :logical   Min.   : 4.0    \n Class :character   1st Qu.:19.0   FALSE:2         1st Qu.:12.0    \n Mode  :character   Median :20.0   TRUE :3         Median :13.0    \n                    Mean   :19.8                   Mean   :13.8    \n                    3rd Qu.:20.0                   3rd Qu.:14.0    \n                    Max.   :22.0                   Max.   :26.0    \n  real_name        \n Length:5          \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nThis is another good validity check to ensure that numeric columns have appropriate minimum and maximum values, etc. Here we see that the the age and solo_nominations columns have reasonable values. In practice you would undertake many more validity checks, but for now this is a good start."
  },
  {
    "objectID": "01-03-data-structures-in-r.html#references",
    "href": "01-03-data-structures-in-r.html#references",
    "title": "3¬† Data Structures in R",
    "section": "References",
    "text": "References\n\n\n\n\nBroman, K. W., & Woo, K. H. (2018). Data organization in spreadsheets. The American Statistician, 72(1), 2‚Äì10. https://doi.org/10.1080/00031305.2017.1375989"
  },
  {
    "objectID": "01-03-data-structures-in-r.html#footnotes",
    "href": "01-03-data-structures-in-r.html#footnotes",
    "title": "3¬† Data Structures in R",
    "section": "",
    "text": "Technically, a data frame can have a single column, but in practice most data frames you encounter will have many columns.‚Ü©Ô∏é\nIn this case, everything in the column would be turned into a character string.‚Ü©Ô∏é\nThe technical type is ‚Äúdouble‚Äù, which is commonly referred to as numeric.‚Ü©Ô∏é\nThis argument for the read_csv() function can also be a pathname to the location of the CSV file on your computer. If you are computing on a Mac, you may need to add ~/ to the beginning of this path name.‚Ü©Ô∏é\nFor data frames only the first six rows are displayed, all the columns are printed regardless of size, and column type is not reported.‚Ü©Ô∏é\nNote that just typing the name of the data frame also allows you to view the data. However, using print allows us more flexibility when the data set is larger.‚Ü©Ô∏é"
  },
  {
    "objectID": "01-04-data-wrangling-with-dplyr.html#piping-the-key-to-using-dplyr",
    "href": "01-04-data-wrangling-with-dplyr.html#piping-the-key-to-using-dplyr",
    "title": "4¬† Data Wrangling with dplyr",
    "section": "4.1 Piping: The Key to Using dplyr",
    "text": "4.1 Piping: The Key to Using dplyr\nRecall that functions work by taking arguments as inputs and then producing an output. For example, the glimpse() function takes the comics data frame as its input.\n\n# View data\nglimpse(comics)\n\nWe could get the same result by using the pipe operator (|&gt;). This operator takes a DATA FRAME, or a tibble, (given immediately before the operator) and uses it as the FIRST argument in the function that comes immediately after the pipe operator.\n\n# The pipe operator makes comics the first argument of the glimpse() function\ncomics |&gt; glimpse()\n\nRows: 23,272\nColumns: 14\n$ character         &lt;chr&gt; \"14\", \"88\", \"99\", \"107\", \"'Spinner\", \"\\\"Thumper\\\" Mo‚Ä¶\n$ comic             &lt;chr&gt; \"Marvel\", \"Marvel\", \"Marvel\", \"Marvel\", \"Marvel\", \"M‚Ä¶\n$ reality           &lt;chr&gt; \"Earth-616\", \"Earth-616\", \"Earth-616\", \"Earth-616\", ‚Ä¶\n$ identity          &lt;chr&gt; \"Secret Identity\", \"Public Identity\", \"Secret Identi‚Ä¶\n$ alignment         &lt;chr&gt; \"Bad\", \"Bad\", \"Neutral\", \"Neutral\", \"Good\", \"Bad\", \"‚Ä¶\n$ eye_color         &lt;chr&gt; NA, \"Blue\", \"Blue\", \"Green\", NA, NA, NA, \"Blue\", NA,‚Ä¶\n$ hair_color        &lt;chr&gt; NA, \"Blond\", NA, NA, NA, \"Bald\", NA, \"White\", NA, \"B‚Ä¶\n$ sex               &lt;chr&gt; \"Female\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"M‚Ä¶\n$ lgbtq             &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"‚Ä¶\n$ lgbtq_note        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ alive             &lt;chr&gt; \"Living\", \"Living\", \"Living\", \"Living\", \"Living\", \"L‚Ä¶\n$ appearances       &lt;dbl&gt; 1, 3, 1, 1, NA, NA, 1, 1, 1, 1, 3, 1, 2, 1, 1, 10, N‚Ä¶\n$ first_appear_date &lt;chr&gt; \"1994, November 01\", \"1994, March 01\", \"1994, Novemb‚Ä¶\n$ first_appear_year &lt;dbl&gt; 1994, 1994, 1994, 1994, 2007, 1965, 1991, 2010, 2011‚Ä¶\n\n\nNote since the glimpse() function did NOT include any additional arguments, we do not include anything between the parentheses after we pipe. Here is another example that illustrate the use of the pipe operator.\n\n# Count number of rows in comics data frame\nnrow(comics)\n\n[1] 23272\n\n# Can be written using the pipe operator as...\ncomics |&gt; nrow()\n\n[1] 23272\n\n\nOne last example will show how we use the additional arguments in the function following the pipe operator. For example, say we wanted to use the print() function to print the tibble/data frame, and we wanted to show all of the columns. The print() function would include not only the name of the tibble we wanted to print, but also the argument width=Inf. Here is the syntax for this:\n\n# Print all columns of comics tibble\nprint(comics, width = Inf)\n\nUsing piping, the syntax would be:\n\n# Print all columns of comics tibble\ncomics |&gt; print(width = Inf)\n\n# A tibble: 23,272 √ó 14\n   character                  comic  reality   identity        alignment\n   &lt;chr&gt;                      &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;           &lt;chr&gt;    \n 1 \"14\"                       Marvel Earth-616 Secret Identity Bad      \n 2 \"88\"                       Marvel Earth-616 Public Identity Bad      \n 3 \"99\"                       Marvel Earth-616 Secret Identity Neutral  \n 4 \"107\"                      Marvel Earth-616 Secret Identity Neutral  \n 5 \"'Spinner\"                 Marvel Earth-616 Secret Identity Good     \n 6 \"\\\"Thumper\\\" Morgan\"       Marvel Earth-616 Secret Identity Bad      \n 7 \"11-Ball\"                  Marvel Earth-616 Secret Identity Bad      \n 8 \"115 (Legion Personality)\" Marvel Earth-616 Secret Identity Neutral  \n 9 \"181 (Legion Personality)\" Marvel Earth-616 Secret Identity Neutral  \n10 \"1X\"                       Marvel Earth-616 Public Identity Good     \n   eye_color hair_color sex    lgbtq lgbtq_note alive  appearances\n   &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;\n 1 &lt;NA&gt;      &lt;NA&gt;       Female No    &lt;NA&gt;       Living           1\n 2 Blue      Blond      Male   No    &lt;NA&gt;       Living           3\n 3 Blue      &lt;NA&gt;       Male   No    &lt;NA&gt;       Living           1\n 4 Green     &lt;NA&gt;       Male   No    &lt;NA&gt;       Living           1\n 5 &lt;NA&gt;      &lt;NA&gt;       Male   No    &lt;NA&gt;       Living          NA\n 6 &lt;NA&gt;      Bald       Male   No    &lt;NA&gt;       Living          NA\n 7 &lt;NA&gt;      &lt;NA&gt;       Male   No    &lt;NA&gt;       Living           1\n 8 Blue      White      Female No    &lt;NA&gt;       Living           1\n 9 &lt;NA&gt;      &lt;NA&gt;       &lt;NA&gt;   No    &lt;NA&gt;       Living           1\n10 &lt;NA&gt;      Blond      Male   No    &lt;NA&gt;       Living           1\n   first_appear_date first_appear_year\n   &lt;chr&gt;                         &lt;dbl&gt;\n 1 1994, November 01              1994\n 2 1994, March 01                 1994\n 3 1994, November 01              1994\n 4 1994, November 01              1994\n 5 2022, November 07              2007\n 6 1965, February 01              1965\n 7 1991, July 01                  1991\n 8 2022, August 10                2010\n 9 2022, July 11                  2011\n10 1940, March 01                 1940\n# ‚Ñπ 23,262 more rows\n\n\nHere, comics will be inputted as the FIRST argument in the print() function, and any additional arguments are simply included in the print() function itself.\n\nIt is a good coding practice to use multiple lines when you are piping rather than putting all the syntax on a single line. When you do this, the pipe operator (|&gt;) needs to come at the end of the line. You can see this in the code below, where the pipe operator is placed at the end of the first line of syntax; not at the beginning of the second line of syntax. Include a line break after every pipe operator you use.\n\n# Print all columns of comics tibble\ncomics |&gt; \n  print(width = Inf)"
  },
  {
    "objectID": "01-04-data-wrangling-with-dplyr.html#common-dplyr-functions-for-data-wrangling",
    "href": "01-04-data-wrangling-with-dplyr.html#common-dplyr-functions-for-data-wrangling",
    "title": "4¬† Data Wrangling with dplyr",
    "section": "4.2 Common dplyr Functions for Data Wrangling",
    "text": "4.2 Common dplyr Functions for Data Wrangling\nHere are some common operations that researchers use to prepare data for analysis (i.e., data preparation, data wrangling, data cleaning) and the corresponding {dplyr} functions.\n\n\n\nCommon data wrangling activities and the corresponding {dplyr} functions.\n\n\n\n\n\n\nData wrangling activity\ndplyr function\n\n\n\n\nSelect a subset of rows from a data frame.\nfilter()\n\n\nSelect a subset of columns from a data frame.\nselect()\n\n\nAdd new columns to a data frame.\nmutate()\n\n\nSort and re-order data in a data frame.\narrange()\n\n\nCompute summaries of columns in a data frame.\nsummarize()\n\n\nGroup the data to carry out computations for each group.\ngroup_by()"
  },
  {
    "objectID": "01-04-data-wrangling-with-dplyr.html#sorting-the-data-arranging",
    "href": "01-04-data-wrangling-with-dplyr.html#sorting-the-data-arranging",
    "title": "4¬† Data Wrangling with dplyr",
    "section": "4.3 Sorting the Data: Arranging",
    "text": "4.3 Sorting the Data: Arranging\nTo answer our initial set of research questions related to early representation of LGBTQ characters in comics, it is useful to sort the data by both LGBTQ status and year of first appearance. The arrange() function sorts the data based on the values within one or more specified columns. The data is ordered based on the column name provided in the argument(s). The syntax below sorts the rows in the comics data frame from earliest to most recent year of first appearance.\n\n# Sort data from earliest to most recent year of first appearance\ncomics |&gt;\n  arrange(first_appear_year)\n\n\n\n  \n\n\n\nHere we see the earliest character in these data (Richard Occult) appeared in 1935. This, however, does not give us the first LBGTQ character. To determine this, we need to sort on LGBTQ status in addition to year of first appearance.\nProviding the arrange() function multiple arguments sort initially by the column name given in first argument, and then by the columns given in subsequent arguments. Here the data are sorted first by LGBTQ status (alphabetically since lgbtq is a character string) and then by year of first appearance.\n\n# Sort data by LGBTQ status and then from earliest to most recent year of first appearance\ncomics |&gt;\n  arrange(lgbtq, first_appear_year)\n\n\n\n  \n\n\n\nBecause No is alphabetically before Yes, the non-LGBTQ characters are printed first. Because only the first 10 rows of a tibble are printed (and all 10 are non-LGBTQ characters), we still can‚Äôt quite answer our research question. If you want to see all of the sorted data or operate on it further, you need to (a) explicitly tell R to print all of the rows, or (b) assign the output into an object which can be viewed and scrolled through by clicking on the object in the RStudio Environment pane.\nTo print all of the rows to the console, we can pipe the sorted data into the print() function, and include the argument N=Inf. Reminder: Best practice is to start a new line after each pipe operator!\n\n# Sort data by LGBTQ status and then from earliest to most recent year of first appearance\n# Print all the rows\ncomics |&gt;\n  arrange(lgbtq, first_appear_year) |&gt;\n  print(N = Inf)\n\n# A tibble: 23,272 √ó 14\n   character   comic reality identity alignment eye_color hair_color sex   lgbtq\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;\n 1 Richard Oc‚Ä¶ DC    New Ea‚Ä¶ Secret ‚Ä¶ &lt;NA&gt;      Grey      Black      Male  No   \n 2 Arthur Pen‚Ä¶ DC    New Ea‚Ä¶ Public ‚Ä¶ Good      Brown     Brown      Male  No   \n 3 Bedivere    DC    New Ea‚Ä¶ &lt;NA&gt;     &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;       Male  No   \n 4 Franklin D‚Ä¶ DC    New Ea‚Ä¶ Public ‚Ä¶ Good      &lt;NA&gt;      Grey       Male  No   \n 5 Gareth      DC    New Ea‚Ä¶ &lt;NA&gt;     &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;       Male  No   \n 6 Gawain      DC    New Ea‚Ä¶ Public ‚Ä¶ Good      &lt;NA&gt;      &lt;NA&gt;       Male  No   \n 7 Guinevere   DC    New Ea‚Ä¶ Public ‚Ä¶ Good      &lt;NA&gt;      Blond      Fema‚Ä¶ No   \n 8 Lady of th‚Ä¶ DC    New Ea‚Ä¶ &lt;NA&gt;     Good      Blue      Blue       Fema‚Ä¶ No   \n 9 Lancelot    DC    New Ea‚Ä¶ Public ‚Ä¶ Good      &lt;NA&gt;      &lt;NA&gt;       Male  No   \n10 Merlin      DC    New Ea‚Ä¶ Secret ‚Ä¶ Neutral   Black     White      Male  No   \n# ‚Ñπ 23,262 more rows\n# ‚Ñπ 5 more variables: lgbtq_note &lt;chr&gt;, alive &lt;chr&gt;, appearances &lt;dbl&gt;,\n#   first_appear_date &lt;chr&gt;, first_appear_year &lt;dbl&gt;\n\n\nAnother way to view the entire set of data is to assign the sorted data into an object and then click on that object in the environment pane.\n\n# Sort data by LGBTQ status and then from earliest to most recent year of first appearance\n# Assign to the object 'sorted_comics'\nsorted_comics = comics |&gt;\n  arrange(lgbtq, first_appear_year)\n\n\n\n\n\n\nClicking the sorted_comics data object in the Environment pane will display the data in the RStudio data viewer.\n\n\n\n\nBased on an examination of the sorted data, we find the first appearance of an LQBTQ character is Jack Casey in 1940.\n\n\n\n\n\n\n4.3.1 Sorting in Descending Order\nRather than scrolling through the data, we could also have sorted the data so that the characters with LGBTQ status of ‚ÄúYes‚Äù are printed first. To do this we want to sort the data initially (using thelgbtq column) in reverse alphabetical order (‚ÄúYes‚Äù followed by ‚ÄúNo‚Äù).\nUse the desc() function on a column name to sort the data in descending order. Here the data are sorted in descending order by LGBTQ status and then by year of first appearance (in ascending order).\n\n# Sort data by LGBTQ status and then from earliest to most recent year of first appearance\ncomics |&gt;\n  arrange(desc(lgbtq), first_appear_year)"
  },
  {
    "objectID": "01-04-data-wrangling-with-dplyr.html#obtain-a-subset-of-rows-filtering",
    "href": "01-04-data-wrangling-with-dplyr.html#obtain-a-subset-of-rows-filtering",
    "title": "4¬† Data Wrangling with dplyr",
    "section": "4.4 Obtain a Subset of Rows: Filtering",
    "text": "4.4 Obtain a Subset of Rows: Filtering\nThere are many times in research applications that an educational scientist will need to select a subset of data cases. This type of application, for example, is quite common when we carry out demographic analyses (e.g., select the special education students, select students on free/reduced-price lunch). To select a subset of rows from a tibble or data frame, we will pipe the data frame we want to select rows from into the filter() function.\nThe argument(s) for the filter() function are logical expressions that will be used to select the rows. For example, suppose we wanted to select the LGBTQ characters (i.e., rows) from the comics data frame. We would need a logical expression that returns a TRUE value for all the LGBTQ characters. One such logical expression is: lgbtq==\"Yes\". Recall that a single equals sign (=) is the assignment operator and that to say ‚Äúis equal to‚Äù, we need to use two equals signs (==). Including this logical expression in the filter() function, the syntax for selecting the LQBTQ characters is then:\n\n# Select the LGBTQ characters\ncomics |&gt;\n  filter(lgbtq == \"Yes\")\n\n\n\n  \n\n\n\nNote that the output from this computation (data for the LGBTQ characters) is only printed to the screen. If you want to keep the filtered data or operate on it further, you need to assign the output into an object.\n\n# Select the LGBTQ characters\nlgbtq_characters = comics |&gt;\n  filter(lgbtq == \"Yes\")\n\n# Count the number of rows\nnrow(lgbtq_characters)\n\n[1] 155\n\n\nWe could have found the same result exclusively using piping; without the interim assignment.\n\n# Select the LGBTQ characters and count the rows\ncomics |&gt;\n  filter(lgbtq == \"Yes\") |&gt;\n  nrow()\n\n[1] 155\n\n\nThe first pipe operator uses the comics data frame in the filter() function to select the LGBTQ characters. This output (only the LGBTQ characters) is then used in the nrow() function to count the number of rows. It is akin to a constant pipeline of chaining functions together (i.e., nrow(filter(comics, lgbtq == \"Yes\"))); the output of a computation is used as the input into the next computation in the pipeline.\nBased on this result (and the results from the earlier glimpse() output), we can now answer our first research question: What percentage of comic characters identify as LQBTQ?\n\n# Compute percentage of LGBTQ characters\n155 / 23272\n\n[1] 0.006660364\n\n\nOnly 0.6% of comic characters identified as LGBTQ (at least as of 2014). This is well below 7.1%, the percentage of U.S. adults who self-identify as lesbian, gay, bisexual, transgender or something other than heterosexual according to a 2022 Gallup Poll. (This is even below the 2012 estimate of 3.5%.) This suggests that the LQBTQ population is likely underrepresented in comic culture.\n\n\n\n\n\n\n4.4.1 Filtering on Multiple Attributes\nYou can filter on multiple attributes by including more than one logical statement in the filter() function. For example, say we wanted to determine if the Pride Movement had an impact on LGBTQ representation in comics. The first Pride parade took place in March 1970, so we could look at the percentage of LGBTQ comic characters introduced prior to 1970 and compare it to the percentage of LGBTQ comic characters introduced in 1970 or later.\nThe syntax below counts the number of LGBTQ comic characters introduced prior to 1970. We also compute the total number of character introduced prior to 1970 to compute the correct percentage.\n\n# Count LGBTQ characters introduced prior to 1970\ncomics |&gt;\n  filter(lgbtq == \"Yes\", first_appear_date &lt; 1970) |&gt;\n  nrow()\n\n[1] 11\n\n# Count all characters introduced prior to 1970\ncomics |&gt;\n  filter(first_appear_date &lt; 1970) |&gt;\n  nrow()\n\n[1] 4002\n\n# Compute percentage\n11 / 4002\n\n[1] 0.002748626\n\n\nOf the 4002 characters introduced prior to 1970, 0.27% identified as LGBTQ.\nHere, when we included multiple logical expressions in the filter() function, separated by a comma, they were linked using the AND (&) operator. This means that both expressions have to evaluate as TRUE to be included. We could also have explicitly used the & operator to link the two statements.\n\ncomics |&gt;\n  filter(lgbtq == \"Yes\", sex == \"Female\")\n\n# Is equivalent to...\ncomics |&gt;\n  filter(lgbtq == \"Yes\" & sex == \"Female\")\n\n\n\n\n\n\nWe can also filter() using the OR (|) operator. This means that if EITHER logical expression included in the filter() function evaluates as TRUE, the row is included in the output. For example, say we wanted to count the number of comic characters who are either female or identify as LGBTQ. The syntax for this would be:\n\n# Count character who are LQBTQ or are female\ncomics |&gt;\n  filter(lgbtq == \"Yes\" | sex == \"Female\") |&gt;\n  nrow()\n\n[1] 5890"
  },
  {
    "objectID": "01-04-data-wrangling-with-dplyr.html#selecting-a-subset-of-columns",
    "href": "01-04-data-wrangling-with-dplyr.html#selecting-a-subset-of-columns",
    "title": "4¬† Data Wrangling with dplyr",
    "section": "4.5 Selecting a Subset of Columns",
    "text": "4.5 Selecting a Subset of Columns\nSuppose a journalist at Lavender Magazine is writing a story about the representation of LGBTQ comic characters and has asked you to create a new dataset for their work that only includes the LGBTQ comic characters. Moreover, this new dataset should also only include the characters‚Äô name, year of first appearance, and the LGBTQ note. To complete this task, we need to select not only a subset of rows from the original data, but also a subset of the columns.\nTo select a subset of columns, we will use the select() function. The argument(s) for this function are the column names of the data frame that you want to select. For example, to select the character, first_appear_year, and lgbtq_note columns from the comics data frame we would use the following syntax:\n\n# Select a subset of columns\ncomics |&gt;\n  select(character, first_appear_year, lgbtq_note)\n\n\n\n  \n\n\n\nWe can combine this column selection with our filtering to select the LGBTQ characters. Note that since the filter() function uses the data in the lgbtq column, we need to apply the filter before we selecting the three columns we want. If we use select() prior to filtering, we will get an error since the column lgbtq was not included in the select() function.\n\n# This order produces and error\ncomics |&gt;\n  select(character, first_appear_year, lgbtq_note) |&gt;\n  filter(lgbtq == \"Yes\")\n\nError in `filter()`:\n‚Ñπ In argument: `lgbtq == \"Yes\"`.\nCaused by error:\n! object 'lgbtq' not found\n\n# This order gets us the data we want\ncomics |&gt;\n  filter(lgbtq == \"Yes\") |&gt;\n  select(character, first_appear_year, lgbtq_note)\n\n\n\n  \n\n\n\nLastly, in order to get this data to the journalist, we need to export the data from R to our computer. The {readr} package includes several functions that allow us to export data from R in a variety of formats. Here we will use the write_csv() function to export the data into a CSV file. This function necessitates that we provide the path and filename for where we want to save the exported CSV file. For example, to write a CSV file called lgbtq-comic-characters.csv to the desktop on a Mac we could use the following syntax:\n\n# Subset data and export it\ncomics |&gt;\n  filter(lgbtq == \"Yes\") |&gt;\n  select(character, first_appear_year, lgbtq_note) |&gt;\n  write_csv(\"/Users/username/Desktop/lgbtq-comic-characters.csv\")\n\nThe syntax to do this on a PC would be something like the following:\n\n# Subset data and export it\ncomics |&gt;\n  filter(lgbtq == \"Yes\") |&gt;\n  select(character, first_appear_year, lgbtq_note) |&gt;\n  write_csv(\"C:\\Users\\username\\Desktop\\lgbtq-comic-characters.csv\")\n\nIn both of the Mac and PC examples, the part of the pathname called username needs to be modified to be the user name for your computer. Typically this is the username you use to login to your computer.\n\n\n\n\n\n\n4.5.1 Helper Functions for select()\nThere are a number of helper functions you can use within the select() function. For example, starts_with(), ends_with(), and contains(). These let you quickly match larger blocks of columns that meet some criterion. The syntax below illustrates a couple of these functions. You can read about other helper functions and see examples here.\n\n# Select all the columns that have a column name that ends in 'r'\ncomics |&gt;\n  select(ends_with(\"r\"))\n\n\n\n  \n\n\n# Select all the columns that have a column name that contains an underscore\ncomics |&gt;\n  select(contains(\"_\"))\n\n\n\n  \n\n\n\n\n\n\n4.5.2 Renaming Columns\nYou can rename a column by using the rename() function. Here we select the character, eye_color, and hair_color columns from the comics data frame and then rename the eye_color and hair_color columns to eye and hair, respectively. Note that this works similar to assignment in that the new column name is to the left of the equal sign.\n\n# Select 3 columns and rename 2 of them\ncomics |&gt;\n  select(character, eye_color, hair_color) %&gt;%\n  rename(eye = eye_color, hair = hair_color)"
  },
  {
    "objectID": "01-04-data-wrangling-with-dplyr.html#create-new-columns-mutating",
    "href": "01-04-data-wrangling-with-dplyr.html#create-new-columns-mutating",
    "title": "4¬† Data Wrangling with dplyr",
    "section": "4.6 Create New Columns: Mutating",
    "text": "4.6 Create New Columns: Mutating\nTo create new columns, we will use the mutate() function. Here we create a new column called num_years based on subtracting the year of the character‚Äôs first appearance from the current year (2022 as of this writing).\n\ncomics |&gt;\n  mutate(\n    num_years = 2022 - first_appear_year\n    )\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nIf you are running this in the console, the num_years column won‚Äôt be displayed because of the default printing options for tibbles; the new column is created, just not displayed. To view it, we can use the print() function with the argument width=Inf, which displays all columns in the tibble.\n\n# Add a new column and display all the columns\ncomics |&gt;\n  mutate(\n    num_years = 2022 - first_appear_year\n    ) |&gt;\n  print(width = Inf)\n\n\n\n\n4.6.1 Creating Multiple New Columns\nYou can create multiple new columns within the same mutate() function. Simply include each new column as an argument. Below we again create num_years, but we also additionally create centered_appearances which computes the difference between the number of appearances for each character and the mean number of appearances.\n\n# Add two new columns\ncomics |&gt;\n  mutate(\n    num_years = 2022 - first_appear_year,\n    centered_appearances = appearances - mean(appearances, na.rm = TRUE)\n    )\n\n\n\n  \n\n\n\nNote that the mean() function includes the optional argument na.rm=TRUE which allows the mean computation when there are NA values; it tells the mean() function to remove the NAs in the computation. (If you didn‚Äôt remove the NAs, the result of the computation would be an NA.)\n\nIf you want to continue to use the newly created columns, you need to assign the output into an object. If you do not assign the output into an object, the data with the new columns is printed to the screen and then the new columns are promptly ‚Äúforgotten‚Äù by R. If you are sure of your syntax, you can re-assign the data into the original object. Here we create the new columns and re-assign this into the comics object.\n\n# Add two new columns and re-assign to 'comics'\ncomics = comics |&gt;\n  mutate(\n    num_years = 2022 - first_appear_year,\n    centered_appearances = appearances - mean(appearances, na.rm = TRUE)\n    )"
  },
  {
    "objectID": "01-04-data-wrangling-with-dplyr.html#computing-summaries-of-data-in-a-column",
    "href": "01-04-data-wrangling-with-dplyr.html#computing-summaries-of-data-in-a-column",
    "title": "4¬† Data Wrangling with dplyr",
    "section": "4.7 Computing Summaries of Data in a Column",
    "text": "4.7 Computing Summaries of Data in a Column\nThe summarize() function is used to compute summaries of data in a given column. Here we compute the mean number of appearances for all comic characters in the data.\n\ncomics |&gt;\n  summarize(\n    M = mean(appearances, na.rm = TRUE)\n    )\n\n\n\n  \n\n\n\n\nBecause the appearance column includes missing values (NAs), we need to include the argument na.rm=TRUE in the mean() function. Including this will compute the mean only using the cases that have values. If there are missing values and the argument is not included, the result of the mean computation will be NA.\n\nThe output from summarize() is a data frame with a single row and one or more columns, depending on how many summaries you computed. Here we computed a single summary so there is only one column. We also named the column M within the summarize() function.\nMultiple summaries can be computed by providing more than one argument to the summarize() function. The output is still a single row data frame, but now there will be multiple columns, one for each summary computation. Here we compute the mean number of appearances for all comic characters in the data and also the standard deviation.\n\ncomics |&gt;\n  summarize(\n    M = mean(appearances, na.rm = TRUE),\n    SD = sd(appearances, na.rm = TRUE)\n    )"
  },
  {
    "objectID": "01-04-data-wrangling-with-dplyr.html#computations-on-groups",
    "href": "01-04-data-wrangling-with-dplyr.html#computations-on-groups",
    "title": "4¬† Data Wrangling with dplyr",
    "section": "4.8 Computations on Groups",
    "text": "4.8 Computations on Groups\nWhile we have leaned that, on average, comic characters appear about 20 times. And, that the variation is quite large (\\(SD=93.8\\)), telling us that there are characters who appear many more times (e.g., Spiderman, Susan Storm, Wonder Woman). Although a useful first step in an analysis, this alone does not answer our research question about how the average number of appearances for comic characters identifying as LQBTQ differ from those who don‚Äôt. To answer this, we need to compute these summary measures for LGBTQ and non-LGBTQ characters separately.\nThe group_by() function groups the data by a specified variable. By itself, this function essentially does nothing. But it is powerful when the grouped output is piped into other functions, such as summarize(). Here we use group_by(lgbtq) to compute the mean number of appearances and also the standard deviation for both LGBTQ and non-LGBTQ characters.\n\ncomics |&gt;\n  group_by(lgbtq) |&gt;\n  summarize(\n    M = mean(appearances, na.rm = TRUE),\n    SD = sd(appearances, na.rm = TRUE)\n    )\n\n\n\n  \n\n\n\nFrom this analysis we can see that characters that identify as LGBTQ appear, on average, about 75 times, while those that do not identify as LGBTQ appear only about 20 times. Both groups of characters have a large standard deviation implying that there is a lot of variation in the number of appearances for both groups.\nYou can also use group_by() with multiple attributes. Simply add additional column names in the group_by() function to create more conditional groups. For example to compute to compute the mean number of appearances and also the standard deviation for both LGBTQ and non-LGBTQ characters conditioned on comic company, we can use the following syntax.\n\ncomics |&gt;\n  group_by(lgbtq, comic) |&gt;\n  summarize(\n    M = mean(appearances, na.rm = TRUE),\n    SD = sd(appearances, na.rm = TRUE)\n    )\n\n\n\n  \n\n\n\nThis produces the summary measures for each of the combinations of the lgbtq and comic variables. So while we see that for both DC and Marvel, LGBTQ characters have more appearances, on average, than non-LGBTQ characters, this difference is more more pronounced for Marvel characters.\nIn one last analysis, we might also compute the sample size associated with these combinations.\n\ncomics |&gt;\n  group_by(lgbtq, comic) |&gt;\n  summarize(\n    M = mean(appearances, na.rm = TRUE),\n    SD = sd(appearances, na.rm = TRUE),\n    N = n()\n    )\n\n\n\n  \n\n\n\nThis added information reminds us that while the average number of appearances for LGBTQ characters is higher than for non-LGBTQ characters (for both DC and Marvel), overwhelming majority of comic characters are non-LGBTQ."
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#some-background",
    "href": "01-05-visualizing-with-ggplot2.html#some-background",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "5.1 Some Background",
    "text": "5.1 Some Background\nThe gg in {ggplot2} stands for grammar of graphics. The grammar of graphics (Wilkinson, 2005) is a formal system of expressive grammatical rules for creating perceivable graphs. The grammar of graphics includes things such as aesthetics, geometries, scales, facets, and guides. Hadley Wickham adopted this grammar into the initial {ggplot} package, which he then re-wrote and updated to create the {ggplot2} package. As you learn how to create plots using {ggplot2}, you will also begin to learn the grammar of graphics. Understanding this grammar will help you describe, and create almost any visualization you can imagine."
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#understanding-the-basic-syntax",
    "href": "01-05-visualizing-with-ggplot2.html#understanding-the-basic-syntax",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "5.2 Understanding the Basic Syntax",
    "text": "5.2 Understanding the Basic Syntax\nPlots in {ggplot2} are built by layering different components. For example, consider the following syntax which creates a scatterplot using the income and CO2 emissions attributes:\n\nggplot(data = gapminder, aes(x = income, y = co2)) + \n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this syntax there are three layers used to create the plot:\n\na global layer,\na geometric layer, and\na theme layer.\n\nThe layers are literally summed together to form the plot. We will look at each of these layers in turn.\n\n\n5.2.1 The Global Layer\nThe first layer (referred to as the global layer) in every plot you create employs the function ggplot(). It contains a reference to the source data (data=) and any global aesthetic mappings (more about this later). The first layer only sets up the plot, it doesn‚Äôt actually plot anything.\n\nggplot(data = gapminder, aes(x = income, y = co2))\n\n\n\n\nThe background layer for the plot is drawn. The domain (x-values) and range (y-values) is based on data in the income and co2 attributes.\n\n\n\n\n\nThe data= argument indicates the source data frame.\nThe aes= argument sets the aesthetic mapping(s).\n\n\n\n\n5.2.2 Aesthetics and Aesthetic Mappings\nAesthetics define what we, as humans, perceive in a given plot; that is the visual properties of a plot. For example, the position of certain elements in the plot (where they are located), or the color or transparency of an element. These aesthetics can be fixed or variable. For example, consider the color of a set of points. If the color is fixed, it would be the same for all the points. If the color is variable, it might be red for some points and blue for other points. Often, this variability in the aesthetic is based on some attribute in our dataset (e.g., points representing Democrats are colored blue, while those representing Republicans are colored red).\nLinking an aesthetic to an attribute in the data, is referred to as an aesthetic mapping. That is because aesthetic mappings map visual properties in the plot (e.g., position, color) to the values in a particular attributes in the data. Aesthetic mappings are specified in an aes() function. In our earlier example syntax, there were two aesthetic mappings that were defined:\n\nIncome values from the data will be mapped to the x-position.\nCO2 values from the data will be mapped to the y-position.\n\nThese mappings were used to define the domain (x-values) and range (y-values) for the blank plot created by the ggplot() global layer.\n\nImportant\nAll aesthetic mappings need to be specified in an aes() function. Because aesthetic mappings use information in the attribute to apply the specified visual properties, the aesthetics (e.g., x=, y=, color=) need to be set to an attribute name in the data frame. For example, both income and co2 are attribute names in the gapminder data object.\n\n\n\n\n5.2.3 Adding Layers\nAfter starting with the global layer, we can modify our visualization by adding layers to the global layer. For example, the layer that includes the function geom_point() is being added to the global layer in the syntax below.\n\n#: fig-cap: Points are added on top of the plot's background layer.\nggplot(data = gapminder, aes(x = income, y = co2)) + \n  geom_point()\n\n\n\n\n\n\n\n\nGeometric layers (geom_*) are used to add things like points, lines, histograms, densities, etc. The geom_point() layer in our plot is actually drawing the points of the scatter plot. These layers draw on the aesthetic mappings defined in the global layer to know where to draw these geometric objects. For example, the data in the income and co2 variables define the x- and y-positions of the points, respectively.\nWhen layers are added they are ‚Äústacked‚Äù on top of previous layers. For example, consider the two sets of syntax below. Each will again create our scatterplot by adding a set of points based on the data in the global layer. Each will also draw a loess smoother (a sort of trend line). However, in the first set of syntax, the smoother is drawn on top of the points, and in the second set of syntax, the points are drawn on top of the loess smoother. (The argument alpha=1 sets the transparency of the smoother to be completely opaque.)\n\n# LEFT: Draw smoother on top of points\nggplot(data = gapminder, aes(x = income, y = co2)) + \n  geom_point() +\n  geom_smooth(alpha = 1)\n\n\n# RIGHT: Draw points on top of smoother\nggplot(data = gapminder, aes(x = income, y = co2)) + \n  geom_smooth(alpha = 1) +\n  geom_point()\n\n\n\n\n\n\n\nLEFT: The smoother is drawn on top of the points. RIGHT: The points are drawn on top of the smoother.\n\n\n\n\n\n\n\nLEFT: The smoother is drawn on top of the points. RIGHT: The points are drawn on top of the smoother.\n\n\n\n\n\nA useful metaphor might be to imagine putting together a collage of many different photographs. Some parts of some of the photographs (especially those you put down first) might be covered up by photographs that you add later.\n\nProtip\nWhen you are creating plots, you might need to switch the order of some of the layers to get the visualization you want.\n\n\n\n\n5.2.4 Good Syntactic Habits\nAs we add multiple layers to build up our plot, it is a good habit to use multiple lines for the syntax. Generally we put one layer on each line.\n\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point()\n\nThe + sign needs to be at the end of the line (not at the beginning). If you are using a script file (which you should be), highlight ALL layers in the plot and click Run to create the plot.\n\nProtip\nIf you write your {ggplot2} syntax across multiple lines, it makes it not only easier to read, but also easier to try out different layers or features. For example, you can comment out lines to remove layers as you are building the plot without losing the syntax you wrote.\nYou can also debug syntax by highlighting all lines up to a given + sign and running the syntax. By subsequently highlighting and running additional lines of syntax, you can often figure out where any errors show up!"
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#global-vs.-local-aesthetic-mappings",
    "href": "01-05-visualizing-with-ggplot2.html#global-vs.-local-aesthetic-mappings",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "5.3 Global vs.¬†Local Aesthetic Mappings",
    "text": "5.3 Global vs.¬†Local Aesthetic Mappings\nAs you learned earlier, we use the aes() function to set aesthetic mappings (i.e., linking attributes to aesthetics). Aesthetic mappings are often set globally (in the initial ggplot() layer). When we include aesthetic mappings in the global layer they are applied to all layers in the plot. For example, the syntax below maps data in the income attribute to the x-position, data from the co2 attribute to the y-position, and data from the region attribute to color for both the loess smoother and the points.\n\n# All aesthetic mappings are global\nggplot(data = gapminder, aes(x = income, y = co2, color = region)) +  \n  geom_smooth() +\n  geom_point()\n\n\n\n\nThe global aesthetic mappings of x- and y-positioning, as well as, color are applied to the smoother and point layers.\n\n\n\n\n\nNotice when we use non-positional aesthetic mappings (e.g., color) a legend, or guide in the grammar of graphics, will be automatically added to our plot for each aesthetic mapping.\n\nAesthetic mappings can also be set locally in a specific layer. Aesthetic mappings set in a specific layer only apply to that particular layer. Below, we continue to globally map data from the income attribute to the x-position, and data from the co2 attribute to the y-position. Both the smoother layer and point layer will utilize that. However, only the point layer will map data from the region attribute to color. Note that regardless of whether the aesthetic mapping is global or local, we still make this mapping inside an aes() function.\n\n# Aesthetic mappings for x- and y-position are global\n# Aesthetic mapping for color only applies to the points layer\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(aes(color = region))\n\n\n\n\nThe global aesthetic mappings of x- and y-positioning are applied to the smoother and point layers. The aesthetic mapping of color is only applied to the point layer."
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#fixed-aesthetics",
    "href": "01-05-visualizing-with-ggplot2.html#fixed-aesthetics",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "5.4 Fixed Aesthetics",
    "text": "5.4 Fixed Aesthetics\nFixed aesthetics assign the exact same value for the visual property for all the observations; it is not based on the data. Fixed aesthetics do not go inside of the aes() function. (Remember: Inside of the aes() function the mapping has to be to an attribute name!) The following syntax uses the following aesthetics:\n\nGlobal aesthetic mapping: Data from the income attribute mapped to the x-position.\nGlobal aesthetic mapping: Data from the co2 attribute mapped to the y-position.\nFixed local aesthetic: All points are given a shape of 22 (filled square with a border). You can see all the shape options here.\nFixed local aesthetic: All points are given a size of 4 (slightly bigger). The default size is 3.\nLocal aesthetic mapping: Data from the region attribute mapped to fill for the points layer only. Because we are using a different shape we use fill= rather than color= to color the observations.\nFixed local aesthetic: All points are given a border color of black (for this shape border color is set using color=).\n\n\n# Aesthetic mappings for x- and y-position are global\n# Aesthetic mapping for fill only applies to the points layer\n# All points are the same shape, size, and border color\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(aes(fill = region), shape = 22, size = 4, color = \"black\")\n\n\n\n\nThe global aesthetic mappings of x- and y-positioning are applied to the smoother and point layers. The aesthetic mapping of color is only applied to the point layer. The point layer also includes several fixed aesthetics."
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#faceting-separate-plots-for-subgroups",
    "href": "01-05-visualizing-with-ggplot2.html#faceting-separate-plots-for-subgroups",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "5.5 Faceting: Separate Plots for Subgroups",
    "text": "5.5 Faceting: Separate Plots for Subgroups\nFaceting creates a separate plot for different levels of a attribute. This is useful, for example, when you want a separate plot for different subgroups. To facet on a single attribute include the facet_wrap() layer. The wiggle, or tilde, (~) sets the attribute to facet on. In the following syntax, we create a separate plot for each region.\n\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point() +\n  facet_wrap(~region)\n\n\n\n\nScatterplot of per person CO2 emissions versus income for 193 countries. This plot is shown for each of the four world regions.\n\n\n\n\nWe can format the output of the facetted plots by setting the number of rows (nrow=) or columns (ncol=). For example, the following syntax outputs all four subplots in a single row.\n\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point() +\n  facet_wrap(~region, nrow = 1)\n\n\n\n\nScatterplot of per person CO2 emissions versus income for 193 countries. This plot is shown for each of the four world regions.\n\n\n\n\n\n\n5.5.1 Facetting on Multiple Variables\nTo facet on multiple attributes, use facet_grid() rather than facet_wrap(). The facet_grid() layer also uses the wiggle or tilde, but in this function we will include an attribute before the tilde and a second attribute after the tilde. This defines the layout of the plot grid, so that the attribute that comes prior to the tilde will be facetted into separate rows, and the attribute that comes after the tilde will be facetted into different columns (i.e., rows ~ columns). The syntax below facets on both world region (rows) and CO2 change (columns).\n\n# Facet: regions in rows; \nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point() +\n  facet_grid(region ~ co2_change)\n\n\n\n\nScatterplot of per person CO2 emissions versus income for 193 countries. This plot is shown for each of the four world regions and whether or not CO2 emissions increased or decreased since 2007."
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#working-with-the-axes",
    "href": "01-05-visualizing-with-ggplot2.html#working-with-the-axes",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "5.6 Working with the Axes",
    "text": "5.6 Working with the Axes\nMany of the functions and layers employed in {ggplot2} syntax is to fine-tune the plot and make it ready for publication. One place this happens is on the axes of the plot. Changing the labels, the limits, or even where breakpoints occur are all things you may want to adjust as you ready a plot for publication.\n\n\n5.6.1 Changing the Axis Label\nTwo commonly used layers are xlab() and ylab(). These layers are used to change the label on the x- and y-axes, respectively. Here we change the axis label on both the x- and y-axes to give more information about the attributes being plotted.\n\n# Change the labels on the x- and y-axis\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point() +\n  xlab(\"Per-person income (in thousands of international dollars, fixed for 2017 prices)\") +\n  ylab(\"Per-person CO2 emissions (in metric tonnes)\")\n\n\n\n\n\n\n\n\n\n\n\n5.6.2 Changing the Axis Limits\nAnother set of commonly used layers are xlim() and ylim(). These layers are used to set the limits on the x-axis and y-axis, respectively. These functions take two values which set the limits on the particular axis. The first value provided is the minimum, and the second value given is the maximum. Here, for example, the x-limits in the plot will be 0 to 125, and the y-axis will be 0 to 50.\n\n# Change the limits on the x- and y-axis\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point() +\n  xlab(\"Per-person income (in thousands of international dollars, fixed for 2017 prices)\") +\n  ylab(\"Per-person CO2 emissions (in metric tonnes)\") +\n  xlim(0, 125) +\n  ylim(0, 50)\n\n\n\n\n\n\n\n\n\n\n\n5.6.3 Fine-Tuning Axis Scales\nThe xlab(), ylab(), xlim() and ylim() functions we used are shortcuts to using scaling layers. The use of scaling layers allows much more fine-tuning and control of the axis scales. There are four different scaling functions you can use depending on which axis (x or y) you want to control and whether the variable plotted along that axis is continuous or discrete. The four functions are:\n\nscale_x_continuous(),\nscale_x_discrete(),\nscale_y_continuous(), and\nscale_y_discrete().\n\nFor example, in our plot, to fine-tune the x-axis we could use scale_x_continuous() since income is a continuous variable and we want to fine-tune the x-axis.\n\n# Fine-tune the x-axis\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point() +\n  scale_x_continuous(\n    name = \"Per-person income (in thousands of international dollars, fixed for 2017 prices)\",\n    limits = c(0, 125),\n    breaks = c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120)\n  ) +\n  ylab(\"Per-person CO2 emissions (in metric tonnes)\") +\n  ylim(0, 50)\n\n\n\n\nScale on the x-axis set to have a label, as well as, specific limits, and break lines.\n\n\n\n\nThe name= option labels the scale‚Äîit replaces xlab(). The limits= argument takes a vector of the minimum and maximum values‚Äîit replaces xlim(). The breaks= option adds break lines on the axis. There are several other options including labels= for labeling the break lines, etc.\n\n\n\n5.6.4 Customizing the Color and Fill\nScaling functions can also be used to fine-tune colors and fills. For these you need to specify the aesthetic, either color or fill, and also the palette you want to use. For example, scale_fill_manual() can be used to manually set the colors when the fill= aesthetic mapping is used, whereas scale_color_manual() can be used to manually set the colors when the color= aesthetic mapping is used.\nIn an earlier example, we used the fill= mapping to fill the points using the region attribute. Because of this we will use the layer scale_fill_manual() to change the fill colors. We use the values= argument to set the colors. Since the region attribute includes four values, we need to give the values= argument a vector of four different colors.\n\n# Specify fill colors using color names\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(shape = 21, aes(fill = region)) +\n  scale_fill_manual(values = c(\"skyblue\", \"plum\", \"orange\", \"chartreuse\"))\n\n\n\n\nColor palette set manually using named color values.\n\n\n\n\nHere, the alphabetic order of the values in the region variable correspond to the order of colors given in the values= argument of scale_color_manual(). In our examples:\n\nAfrica ‚Äì&gt; ‚Äúskyblue‚Äù\nAmericas ‚Äì&gt; ‚Äúplum‚Äù\nAsia ‚Äì&gt; ‚Äúorange‚Äù\nEurope ‚Äì&gt; ‚Äúchartreuse‚Äù\n\nThe color names used are built-in color names that R knows. There are 657 different named colors that you can use. To see them all, use the colors() function (with no arguments).\n\n\n5.6.4.1 Specifying Colors: RGB Color Model\nColors can also be defined using an RGB color model. This model uses a triplet of values to indicate the intensity of red (R), green (G), and blue (B) hues in the color. Each value in the triplet takes a value from 0 (none of that hue) to 255 (complete hue). For example, the color ‚Äúskyblue‚Äù is equivalent to the RGB value of (135, 206, 235), where the triplet values correspond to:\n\nRed: 135\nGreen: 206\nBlue: 235\n\nUsing the RGB color model, we can obtain 16,777,216 different colors! To specify a color using the RGB color model, we use the rgb() function. This function takes the arguments red=, green= and blue=. We also need to indicate the maximum color value1, in our case 255 using maxColorValue=. The syntax to re-create the colors from the previous plot using the RGB color model is shown below.\n\n# Specify fill colors using RGB color model\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(shape = 21, size = 4, aes(fill = region)) +\n  scale_fill_manual(\n    values = c(\n      rgb(red = 135, green = 206, blue = 235, maxColorValue = 255), #skyblue\n      rgb(red = 221, green = 160, blue = 221, maxColorValue = 255), #plum\n      rgb(red = 255, green = 165, blue = 0,   maxColorValue = 255), #orange\n      rgb(red = 127, green = 255, blue = 0,   maxColorValue = 255) #chartreuse\n    )\n  )\n\n\n\n\nColor palette set manually using RGB triplets.\n\n\n\n\n\n\n\n5.6.4.2 Specifying Colors: HEX\nColors can also be defined using hexadecimal (hex) notation. This notation takes the three RGB color model values (from 0‚Äì255) and converts each of those to a two-digit base-16 value. For example ‚Äúskyblue‚Äù, which in the RGB color model was (135, 206, 235) in hex is:\n\nRed: 135 (RGB) ‚Äì&gt; 87 (hex)\nGreen: 206 (RGB) ‚Äì&gt; CE (hex)\nBlue: 235 (RGB) ‚Äì&gt; EB (hex)\n\nThere are many websites that will convert between RGB values and hex, including Google. (Just type the following search string into Google: ‚Äúconvert 135, 206, 235 to hex‚Äù.) The hex notation for color is the six-digit value of the three concatenated two-digit color values. So skyblue is denoted:\n\nSkyblue: #87CEEB\n\nWhen we specify hex notation as a color in R, the six-digit color value is always preceeded by a hashtag (#). This is given as a character string instead of a color name. So to specify the colors in our plot using hex notation, the syntax is as follows.\n\n# Specify fill colors using hex notation\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(shape = 21, size = 4, aes(fill = region)) +\n  scale_fill_manual(values = c(\"#87CEEB\", \"#DDA0DD\", \"#FFA500\", \"#7FFF00\"))\n\n\n\n\nColor palette set manually using hex notation.\n\n\n\n\n\nUsing lower-case letters in the hex notation would also work. Just remember to always include a hashtag when specifying hex."
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#selecting-a-color-palette",
    "href": "01-05-visualizing-with-ggplot2.html#selecting-a-color-palette",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "5.7 Selecting a Color Palette",
    "text": "5.7 Selecting a Color Palette\nSelecting a color palette can be challenging. It should be aesthetically pleasing, but needs to convey the differences and nuances in the data that you are using color to display. In addition, roughly 8% of males and 0.5% of females have some form of color vision deficiency which will affect how they see and interpret the plot. Here are a few resources for thinking about color palettes:\n\nPicking a Colour Scale for Scientific Graphics\nColor Universal Design (CUD): How to Make Figures and Presentations that are Friendly to Colorblind People\nColor Palettes in R\n\nIf you are creating visualizzations for a particular organization, there may be a palette of official colors associated with their brand. For example, the University of Minnesota‚Äôs two official primary colors in hex notation (for electronic display) are:\n\n#ffcc33 (gold)\n#7a0019 (maroon)\n\nYou can see more on the University Relations Colors and Type page. They also have a palette of complementary colors available.\n\n\n5.7.1 Pre-Selected Color Palettes\nThere are several ‚Äúbuilt-in‚Äù palettes available for use in {ggplot2}.\n\n\n\n\n\n\n\n\nFill Scale\nColor Scale\nDescription\n\n\n\n\nscale_fill_hue()\nscale_color_hue()\nColors evenly spaced around the color wheel\n\n\nscale_fill_grey()\nscale_color_grey()\nGrey scale palette\n\n\nscale_fill_brewer()\nscale_color_brewer()\nColorBrewer palettes\n\n\n\nIf you do not specify the colors to use, the plots created in{ggplot2} will default to a palette that is evenly spaced around the color wheel. To use one of the other built-in palettes, we use the appropriate scale from the table above instead of the scale_fill_manual() or scale_color_manual() layer.\nFor example, to use a greyscale color palette, useful if you are printing in black-and-white, we can use the following syntax:\n\n# Specify greyscale for fill colors\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(shape = 21, size = 4, aes(fill = region)) +\n  scale_fill_grey()\n\n\n\n\nGreyscale color palette good for black-and-white printing.\n\n\n\n\nAnother set of built-in palettes are the Brewer palettes. These paleetes were chosen by Cynthia Brewer, a cartographer and artist, to be both colorblind friendly and aesthetically pleasing. Moreover, the palettes were designed to help humans make sense of the visualized data based on how we perceive the colors that are displayed. Martin Krzywinski has written a very readable introduction to the Brewer color palettes that I recommend.\nShe has palettes for three different types of data\n\nQualitative/Categorical: Colors do not have a perceived order\nSequential: Colors have a perceived order and perceived difference between successive colors is uniform\nDiverging: Two back-to-back sequential palettes starting from a common color (e.g., for Likert scale data)\n\n\n\n\n\n\n\n\n\n\nTo use one of Cynthia Brewer‚Äôs color palettes, we employ scale_color_brewer() or scale_fill_brewer(). Within these functions, we need to specify a palette using the palette= argument. The palette names can be found at https://colorbrewer2.org/. In our example, the region values constitute qualitative/categorical data, so we could choose any of the qualitative color palettes. To use the qualitative color palette called ‚ÄúSet1‚Äù we use:\n\n# Specify Brewer color palette (qualitative, Set 1)\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(shape = 21, size = 4, aes(fill = region)) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nSet1 qualitative color palette from the Brewer color palette.\n\n\n\n\nFinally, we note that there are several packages that can extend the number of built-in color palettes. Some of my favorite extension packages include:\n\n{ggthemes} includes palettes used by FiveThirtyEight and Tableau [see here];\n{wesanderson} includes palettes based on Wes Anderson movies [see here]; and\n{nationalparkcolors} includes palettes based on National Park posters and images [see here; this needs to be installed from GitHub].\n\nEmil Hvitfeldt has also put together a package called {paletteer}, that includes a comprehensive collection of color palettes in R that can be called using a common interface [see here].\n\n\nCode\n# Load libraries\nlibrary(ggthemes)\n\n# FiveThirtyEight fill colors\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(shape = 21, size = 4, aes(fill = region)) +\n  scale_fill_fivethirtyeight()\n\n\n\n\n\nFiveThirtyEight color palette from the {ggthemes} package.\n\n\n\n\n\n\nCode\n# Load libraries\nlibrary(wesanderson)\n\n\n# BottleRocket2 fill colors\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(shape = 21, size = 4, aes(fill = region)) +\n  scale_fill_manual(values = wes_palette(name = \"BottleRocket2\", n = 4))\n\n\n\n\n\nBottleRocket2 color palette from the {wesanderson} package.\n\n\n\n\n\n\nCode\n# Load libraries\nlibrary(nationalparkcolors)\n\n\n# Acadia fill colors\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(shape = 21, size = 4, aes(fill = region)) +\n  scale_fill_manual(values = park_palette(\"Acadia\", n = 4))\n\n\n\n\n\nAcadia color palette from the {nationalparks} package."
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#customizing-the-legendguide",
    "href": "01-05-visualizing-with-ggplot2.html#customizing-the-legendguide",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "5.8 Customizing the Legend/Guide",
    "text": "5.8 Customizing the Legend/Guide\nScaling functions can also be used to change the name and labels in the legend or guide. When we used the color scales, the guide included both the name of the attribute being plotted, and each value of the attribute along with its corresponding fill color. To change the , We can include the argument name= to edit the text of the attribute name in the guide. We can also use labels= to edit the text for each of the attribute vaslues. For example, below we change the text of the attribute in the guide to read ‚ÄúWorld Region‚Äù and the text of ‚ÄúAmericas‚Äù to instead read ‚ÄúThe Americas‚Äù. Note that because there are four attribute values, the labels= argument needs to have a vector with four values; we can‚Äôt only include the text we want to change.\n\n# Specify Brewer color palette (qualitative, Set 1)\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(shape = 21, size = 4, aes(fill = region)) +\n  scale_fill_manual(\n    name = \"World Region\",\n    values = c(\"#87CEEB\", \"#DDA0DD\", \"#FFA500\", \"#7FFF00\"),\n    labels = c(\"Africa\", \"The Americas\", \"Asia\", \"Europe\")\n  )\n\n\n\n\nSet1 qualitative color palette from the Brewer color palette."
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#themes-changing-the-look-of-your-plot",
    "href": "01-05-visualizing-with-ggplot2.html#themes-changing-the-look-of-your-plot",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "5.9 Themes: Changing the Look of Your Plot",
    "text": "5.9 Themes: Changing the Look of Your Plot\nThere are several ‚Äúbuilt-in‚Äù themes that you can use to change the look of your plot: theme_grey(), theme_minimal(), theme_linedraw(), theme_light(), theme_dark(), theme_minimal(), theme_classic(), theme_void(), and theme_test(). The default theme is theme_grey(). Here we use the layer theme_minimal() to modify the look of our plot. This theme uses a minimal black-and-white background (rather than grey).\n\n# Using the `theme_minimal()` layer to update the look of the plot\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(shape = 21, size = 4, aes(fill = region)) +\n  scale_fill_manual(\n    name = \"World Region\",\n    values = c(\"#87CEEB\", \"#DDA0DD\", \"#FFA500\", \"#7FFF00\"),\n    labels = c(\"Africa\", \"The Americas\", \"Asia\", \"Europe\")\n  ) +\n  theme_minimal()\n\n\n\n\nUsing the theme_minimal() layer to update the look of the plot.\n\n\n\n\n\n\n5.9.1 The ggthemes Package\nThe {ggthemes} package includes 20 additional themes that you can use to style your plot (see here to view the different themes available). Here I use a theme similar to that from plots that appear in the Wall Street Journal.\n\n# Load library\nlibrary(ggthemes)\n\n# Using the `theme_wsj()` layer to update the look of the plot\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(shape = 21, size = 4, aes(fill = region)) +\n  scale_fill_manual(\n    name = \"World Region\",\n    values = c(\"#87CEEB\", \"#DDA0DD\", \"#FFA500\", \"#7FFF00\"),\n    labels = c(\"Africa\", \"The Americas\", \"Asia\", \"Europe\")\n  ) +\n  theme_wsj()\n\n\n\n\nUsing the theme_wsj() layer from the {ggthemes} package to update the look of the plot to look like plots in the Wall Street Journal.\n\n\n\n\n\n\n\n5.9.2 Customizing a Theme\nThe theme() layer can be used to change every element in the plot (e.g., grid lines, font, color, etc.). See here for more detail. In the syntax below, we call theme_minimal() to get the minimal black-and-white them from before, and the use arguments in the theme() layer to change the font face and color on the axes labels.\n\n# Using the `theme_minimal()` layer to update the look of the plot\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(shape = 21, size = 4, aes(fill = region)) +\n  scale_fill_manual(\n    name = \"World Region\",\n    values = c(\"#87CEEB\", \"#DDA0DD\", \"#FFA500\", \"#7FFF00\"),\n    labels = c(\"Africa\", \"The Americas\", \"Asia\", \"Europe\")\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_text(face = \"bold\", color = \"blue\"),\n    axis.title.y = element_text(face = \"italic\")\n  )\n\n\n\n\nYou can customize almost any part of the theme using the theme() layer. In this plot we changed the font face and color on the axes labels.\n\n\n\n\nThe theme() function can also be used to create your own {ggplot2} theme. This is useful if you always want your plots to look a certain way (e.g., to fit an organizations style guide) or are always using the same modifications to the theme in your plots. Below, I create a theme called theme_andy() that\n\n# Create theme_andy()\ntheme_andy = function() {\n  theme_minimal() +\n  theme(\n    # add border\n    panel.border = element_rect(colour = \"#7a0019\", fill = NA, linetype = 1),\n    # modify grid\n    panel.grid.major.x = element_line(color = \"#7a0019\", linetype = 1, size = 0.25),\n    panel.grid.minor.x = element_line(color = \"#ffb71e\"),\n    panel.grid.major.y = element_line(color = \"#7a0019\", linetype = 1, size = 0.25),\n    panel.grid.minor.y = element_line(color = \"#ffb71e\"),\n    # modify text, axis and color\n    axis.text = element_text(colour = \"#5b0013\", face = \"italic\", family = \"Times New Roman\"),\n    axis.title = element_text(colour = \"#5b0013\", family = \"Times New Roman\"),\n    axis.ticks = element_line(colour = \"#5b0013\"),\n    # legend at the bottom\n    legend.position = \"bottom\"\n  )\n}\n\n# Use theme_andy() to theme the plot\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(shape = 21, size = 4, aes(fill = region)) +\n  scale_fill_manual(\n    name = \"World Region\",\n    values = c(\"#87CEEB\", \"#DDA0DD\", \"#FFA500\", \"#7FFF00\"),\n    labels = c(\"Africa\", \"The Americas\", \"Asia\", \"Europe\")\n  ) +\n  theme_andy()\n\n\n\n\nYou can also create your own theme to use with any plot."
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#ggplot2-extension-packages",
    "href": "01-05-visualizing-with-ggplot2.html#ggplot2-extension-packages",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "5.10 {ggplot2} Extension Packages",
    "text": "5.10 {ggplot2} Extension Packages\nThere are several packages that extend the functionality of {ggplot2}. Many of these packages are listed at the ggplot2 Extensions page.\n\n\n\n\n\n\n\n\n\nOne useful extension package is the {scales} package. The functions and layers in this package are useful for transforming and labeling scales that are used to create your plot. For example, one of the functions in the {scales} package. label_number(), can be used to add a character prior to, or after all the numericval values labelled on an axis (very useful if the attribute being plotted is dollars). There is a nice web tutorial on using the {scales} package here.\nHere we add\n\n# Load library\nlibrary(scales)\n\n# Use the `{scales}` package to add $ in front of labels and k at the end on the x-axis\nggplot(data = gapminder, aes(x = income, y = co2)) +  \n  geom_smooth() +\n  geom_point(shape = 21, size = 4, aes(fill = region)) +\n  scale_fill_manual(\n    name = \"World Region\",\n    values = c(\"#87CEEB\", \"#DDA0DD\", \"#FFA500\", \"#7FFF00\"),\n    labels = c(\"Africa\", \"The Americas\", \"Asia\", \"Europe\")\n  ) +\n  scale_x_continuous(\n    name = \"Income (in international dollars, fixed for 2017 prices)\",\n    labels = label_number(prefix = \"$\", suffix = \"k\")\n  ) +\n  theme_minimal()\n\n\n\n\nThe {scales} package helps format labels on the axes. Here we have used it to add a dollar sign in front of labels on the x-axis."
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#captions-and-figure-numbering",
    "href": "01-05-visualizing-with-ggplot2.html#captions-and-figure-numbering",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "5.11 Captions and Figure Numbering",
    "text": "5.11 Captions and Figure Numbering\n\nIt is easier to use a word-processor (e.g., MS Word) to add the figure title and caption than to try and get it formatted correctly using R. This is especially true when trying to mimic the APA format.\n\nHere is one workflow for saving plots, importing the saved plot into your word-processing application, and adding a figure number and caption.\n\nCreate your plot in R.\nExport it (Click the Export button above the plot in RStudio.\nSelect Save as Image...\nClick `Directory to indicate where you want to save the image\nGive the plot a name in the File name box\nChange the height and width values until you have a good aspect ratio for the plot. You can preview the image to see that it looks good.\nClick Save\nIn Word or Google Docs, import the image from where you saved it.\nResize the image to take up less space if you can. (It needs to be readable, but not too big.)\nAdd the figure number and caption using your word-processing document. :::"
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#using-piping-with-ggplot2",
    "href": "01-05-visualizing-with-ggplot2.html#using-piping-with-ggplot2",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "5.12 Using Piping with {ggplot2}",
    "text": "5.12 Using Piping with {ggplot2}\nSince output from the piping operator produces a data frame, we can pipe the data into the ggplot() global layer rather than using the argument data=. For example:\n\n# Use piping to indicate the data\ngapminder |&gt;\n  ggplot(aes(x = income, y = co2)) +  \n    geom_smooth() +\n    geom_point() \n\nThis is useful if you are wrangling data and want to see a plot. Here for example we filter to obtain the African countries and create the plot using only those countries. Note that since we only have observations from one region, we can now change the fill= to a fixed aesthetic.\n\ngapminder |&gt;\n  filter(region == \"Africa\") |&gt;\n  ggplot(aes(x = income, y = co2)) +  \n    geom_smooth() +\n    geom_point(shape = 21, size = 4, fill = \"#87CEEB\") +\n    theme_minimal()\n\n\n\n\nUsing {dplyr} functions and piping to plot athe African countries."
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#practice-practice-practice",
    "href": "01-05-visualizing-with-ggplot2.html#practice-practice-practice",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "5.13 Practice, Practice, Practice",
    "text": "5.13 Practice, Practice, Practice\nThe key to becoming a {ggplot2} ninja is to practice by creating visualizations. They don‚Äôt have to be fancy! Even by creating simple visualizations, you will hone your skills. As you become more confident try changing the color, or the labels, or really anything.\nIf you are unsure where to start, one practice method I have found helpful is to try and re-create a plot that someone else has already created. There are many datasets available online that people have created plots for. One source of these is the Tidy Tuesday Repository.\nThere are also several resources to help you get started and continue your gg_journey! Here are a few I have found helpful:\n\nData visualization with ggplot2 cheatsheet : A one-page (front and back) cheatsheet of ggplot2 syntax with pictures https://www.rstudio.com/resources/cheatsheets/\nCookbook for R: Web-based version of Winston Chang‚Äôs R Graphics Cookbook http://www.cookbook-r.com/Graphs/ (The UMN library has electronic access to the actual book. Just search for ‚ÄúR Graphics Cookbook‚Äù and log-in with your x500.)\nData Visualization: A Practical Introduction: Online book about data viz using ggplot2 https://socviz.co/\n\nHappy Plotting!!! üìà ü•≥"
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#references",
    "href": "01-05-visualizing-with-ggplot2.html#references",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "References",
    "text": "References\n\n\n\n\nWilkinson, L. (2005). The grammar of graphics (2nd ed.). Springer."
  },
  {
    "objectID": "01-05-visualizing-with-ggplot2.html#footnotes",
    "href": "01-05-visualizing-with-ggplot2.html#footnotes",
    "title": "5¬† Visualizing Data with ggplot2",
    "section": "",
    "text": "Computer scientists often use a 0‚Äì1 scale of intensity rather than a 0‚Äì255. In that case, our color of ‚Äúskyblue‚Äù would have an RGB triplet of (0.5294118 0.8078431 0.9215686). Here the original RGB triplet values are divided by 255.‚Ü©Ô∏é"
  },
  {
    "objectID": "02-00-regression-basics.html",
    "href": "02-00-regression-basics.html",
    "title": "Regression Basics",
    "section": "",
    "text": "In this unit we will introduce the ideas of regression. You will learn about using the simple regression model to describe the relationship between two quantitative variables. You will also learn how the parameters for this model are estimated and how correlation and regression are related. Finally you will learn how to carry out statistical inference on the regression parameters."
  },
  {
    "objectID": "02-01-simple-regression-description.html#data-exploration",
    "href": "02-01-simple-regression-description.html#data-exploration",
    "title": "6¬† Simple Linear Regression‚ÄîDescription",
    "section": "6.1 Data Exploration",
    "text": "6.1 Data Exploration\nAny analysis should start with an initial exploration of the data. During this exploration, you should examine each of the variables that you will be including in the regression analysis. This will help you understand the results you get in later analyses, and will also help foreshadow potential problems with the analysis. This blog post describes initial ideas of data exploration reasonably well. You could also refer to almost any introductory statistics text for additional detail.\nIt is typical to begin by exploring the distribution of each variable used in the analysis separately. These distributions are referred to as marginal distributions. After that, it is appropriate to explore the relationships between the variables.\n\n\n6.1.1 Marginal Distribution of Income\nTo begin this exploration, we will examine the marginal distribution of employee incomes. We can plot a marginal distribution using functionality from the {ggplot2} package.\n\nggplot(data = city, aes(x = income)) +\n  stat_density(geom = \"line\") +\n  theme_bw() +\n  xlab(\"Income (in thousands of dollars)\") +\n  ylab(\"Probability density\")\n\n\n\n\nDensity plot of employee incomes.\n\n\n\n\nThis plot suggests that the distribution of employee incomes is unimodal and most of the incomes are between roughly $50,000 and $70,000. The smallest income in the sample is about $25,000 and the largest income is over $80,000. This indicates there is a fair amount of variation in the data.\nTo further summarize the distribution, it is typical to compute and report summary statistics such as the mean and standard deviation. One way to compute these values is to use functions from the {dplyr} library.\n\ncity |&gt;\n  summarize(\n    M = mean(income),\n    SD = sd(income)\n    )\n\n\n\n  \n\n\n\nDescribing this variable we might write,\n\nThe marginal distribution of income is unimodal with a mean of approximately $53,700. There is variation in employees‚Äô salaries (SD = $14,500).\n\n\n\n\n6.1.2 Marginal Distribution of Education Level\nWe will also examine the marginal distribution of the education level variable.\n\n# Plot\nggplot(data = city, aes(x = education)) +\n  stat_density(geom = \"line\") +\n  theme_bw() +\n  xlab(\"Education level\") +\n  ylab(\"Probability density\")\n\n\n\n\nDensity plot of employee education levels.\n\n\n\n\n\n# Summary statistics\ncity |&gt;\n  summarize(\n    M = mean(education),\n    SD = sd(education)\n    )\n\n\n\n  \n\n\n\nAgain, we might write,\n\nThe marginal distribution of education is unimodal with a mean of 16 years. There is variation in employees‚Äô level of education (SD = 4.4).\n\n\n\n\n6.1.3 Relationship Between Variables\nAlthough examining the marginal distributions is an important first step, those descriptions do not help us directly answer our research question. To better understand any relationship between income and education level we need to explore how the distribution of income differs as a function of education. To do this, we will create a scatterplot of income versus education.\n\nggplot(data = city, aes(x = education, y = income)) +\n  geom_point() +\n  theme_bw() +\n  xlab(\"Education (in years)\") +\n  ylab(\"Income (in U.S. dollars)\")\n\n\n\n\nScatterplot displaying the relationship between employee education levels and incomes.\n\n\n\n\nThe plot suggests a relationship (at least for these employees) between level of education and income. When describing the relationship we want to touch on four characteristics of the relationship:\n\nFunctional form of the relationship\nDirection\nStrength\nObservations that do not fit the trend (outliers)"
  },
  {
    "objectID": "02-01-simple-regression-description.html#statistical-model-mathematical-description-of-the-data",
    "href": "02-01-simple-regression-description.html#statistical-model-mathematical-description-of-the-data",
    "title": "6¬† Simple Linear Regression‚ÄîDescription",
    "section": "6.2 Statistical Model: Mathematical Description of the Data",
    "text": "6.2 Statistical Model: Mathematical Description of the Data\nSince the relationship‚Äôs functional form seems reasonably linear, we will use a linear model to describe the data. We can express this model mathematically as,\n\\[\nY_i = \\beta_0 + \\beta_1(X_i) + \\epsilon_i\n\\]\nIn this equation,\n\n\\(Y_i\\) is the outcome/response value; it has an \\(i\\) subscript because it can vary across cases/individuals.\n\\(\\beta_0\\) is the intercept of the line that best fits the data; it does not vary across individuals.\n\\(\\beta_1\\) is the slope of the line that best fits the data; it does not vary across individuals.\n\\(X_i\\) is the predictor value; it has an \\(i\\) subscript because it can vary across cases/individuals.\n\\(\\epsilon_i\\) is the error term; it has an \\(i\\) subscript because it can vary across cases/individuals.\n\nThe linear statistical model (i.e., the regression model) can be separated into two components: a systematic (or fixed) component and a random (or stochastic) component.\n\\[\nY_i = \\underbrace{\\beta_0 + \\beta_1(X_i)}_{\\substack{\\text{Systematic} \\\\ \\text{(Fixed)}}} + \\underbrace{\\epsilon_i}_{\\substack{\\text{Random} \\\\ \\text{(Stochastic)}}}\n\\]\n\n\n6.2.1 Systematic Part of the Statistical Model\nThe systematic (fixed) part of the equation gives the mean value of Y given a particular X-value. The mean of Y given a particular X-value is referred to as a conditional mean of Y. The notation for the conditional mean of Y given a particular X value is \\(\\mu_{Y \\vert X_i}\\). We express the systematic part of the linear regression model mathematically as,\n\\[\n\\mu_{Y \\vert X_i} = \\beta_0 + \\beta_1(X_i)\n\\]\nNote that the systematic part of the equation does not include the error term. The error term is a part of the random component of the model. Statisticians also use \\(\\hat{Y_i}\\) to indicate the conditional mean of Y at a particular X. Thus, the systematic part of the linear regression model can also be written as,\n\\[\n\\hat{Y_i} = \\beta_0 + \\beta_1(X_i)\n\\]\nThe terms \\(\\beta_0\\) and \\(\\beta_1\\) in the systematic part of the model are referred to as the regression parameters. One of the primary goals of a regression analysis is to estimate the values of the regression parameters (i.e., the intercept and slope terms). Thus the systematic part of the regression model is a description, in mathematical form, of how the conditional mean Y is related to X. The equation here indicates that the conditional mean of Y (\\(\\hat{Y_i}\\)) is a linear function of X. This implies that the conditional mean value of Y differs by a constant amount for a constant difference in X.\nFor example, the difference between the mean income for those employees who have 10 years of education and those that have 11 years of education is the same as the difference between the mean income for those employees who have 17 years of education and those that have 18 years of education. Or, the difference between the mean income for those employees who have 4 years of education and those that have 8 years of education is the same as the difference between the mean income for those employees who have 20 years of education and those that have 24 years of education.\n\n\n\n6.2.2 Visual Representation of the Regression Model\nTo help better understand the model, consider the following plot:\n\n\n\n\n\nPlot displaying conditional distribution of Y at several X values. The OLS fitted regression line (dotted) is also shown. The red points show the mean value of Y for each conditional distribution.\n\n\n\n\nAt each value of X there is a distribution of Y. For example, there would be a distribution of incomes for the employees with an education level of 10 years (in the population). There would be another distribution of incomes for the employees with an education level of 11 years (in the population). And so on. These are the conditional distributions of Y.\nEach conditional distribution of Y has a mean; the conditional mean or \\(\\hat{Y_i}\\). These conditional means can be connected using a line. This is what it means to be able to express the conditional mean of Y as a linear function of X, or to say that the relationship between X and Y is linear.\n\n\n\n6.2.3 Random Part of the Statistical Model\nFrom the visual representation of the model, we can see that there is a distribution of Y-values at each X-value; this is represented by the normal distributions in the picture. In our example, there are many employees who have the same education level, but have different incomes. The error term in the statistical model accounts for this variation in Y for those cases that have the same X-value. Mathematically we can understand this by re-writing the statistical model, substituting \\(\\mu_{Y \\vert X_i}\\) into the systematic part of the model.\n\\[\n\\begin{split}\nY_i &= \\beta_0 + \\beta_1(X_i) + \\epsilon_i \\\\\nY_i &= \\mu_{Y \\vert X_i} + \\epsilon_i\n\\end{split}\n\\]\nThis equation implies that each observed Y-value is the sum of the conditional mean value of Y (which is based on the X-value) and some residual (or error) term. Re-arranging the terms, we can mathematically express the residual term as,\n\\[\n\\epsilon_i = Y_i - \\mu_{Y \\vert X_i}\n\\]\nOr, using the \\(\\hat{Y_i}\\) notation,\n\\[\n\\epsilon_i = Y_i - \\hat{Y_i}\n\\]\nTo compute an observation‚Äôs residual, we compute the difference between the observation‚Äôs Y-value and its conditional mean value. When the observed value of Y is larger than the conditional mean value of Y the residual term will be positive (underprediction). If the observed value of Y is smaller than the conditional mean value of Y the residual term will be negative (overprediction).\nTo further understand the residual term, consider the plot below. This figure shows the relationship between education and income we plotted earlier, and also includes the regression line.\n\n\n\n\n\nScatterplot displaying the relationship between employee education levels and incomes. The OLS fitted regression line is also displayed.\n\n\n\n\nConsider the three employees that have an education level of 10 years. The conditional mean income for these three employees is approximately $37,800. This is denoted by the blue point. Remember, the conditional means are on the regression line. The error (residual) term allows for a discrepancy between the conditional mean of Y and the observed Y. In other words, none of these three employees have an actual income of $37,800. The residual represents the difference between the employee‚Äôs observed income and the conditional mean income based on their education level.\nGraphically, the residual is represented by the vertical distance between the line and a given point on the scatterplot. Some of those points are above the line (they have a positive residual) and some are below the line (they have a negative residual). Also note that for some observations the error term is smaller than for others."
  },
  {
    "objectID": "02-01-simple-regression-description.html#estimating-parameters-in-the-regression-model",
    "href": "02-01-simple-regression-description.html#estimating-parameters-in-the-regression-model",
    "title": "6¬† Simple Linear Regression‚ÄîDescription",
    "section": "6.3 Estimating Parameters in the Regression Model",
    "text": "6.3 Estimating Parameters in the Regression Model\nThe regression model describes the relationship between Y-values and X-values in the population. Every term in the model denoted using a Greek letter is an unknown parameter in this model. In the model we have written there are three unknown parameters denoted in the model: the intercept term (\\(\\beta_0\\)), the slope term (\\(\\beta_1\\)), and the residual term (\\(\\epsilon_i\\)).1\nIn most statistical analyses, you will use a sample of data (not the entire population) to estimate the parameter values. Because a sample is only a subset of the population, the values we obtain for the parameters are imperfect estimates. To denote that the parameters are sample-based estimates, we add a hat to each parameter we are estimating. For example, estimates of the parameter estimates of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) (referred to as regression coefficients) are typically reported in a regression analysis and should include hats.\nApplied researchers and statisticians tend to focus their analysis on the systematic (fixed) part of the model, and are thus often most interested in the values of the regression coefficients. After fitting the model to data, the estimated conditional means can be expressed mathematically as:\n\\[\n\\hat\\mu_{Y \\vert X_i} = \\hat\\beta_0 + \\hat\\beta_1(X_i)\n\\]\nOr, using the notation \\(\\hat{Y}_i\\) rather than \\(\\hat\\mu_{Y \\vert X_i}\\), as:\n\\[\n\\hat{Y_i} = \\hat\\beta_0 + \\hat\\beta_1(X_i)\n\\]\nWe use the hats when we are indicating sample-based estimates of these values, so hats should be used when you are reporting the values obtained after using a sample of data to obtain the values.2 The two estimated parameters of interest here (\\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)) are referred to as the estimated regression coefficients, and this equation is often referred to as the fitted regression equation, or simply the fitted equation.\n\n\n6.3.1 Estimating Residuals\nNote that we can also use the estimated regression coefficients to obtain estimates for the residuals, often referred to as the observed residuals. Here we make use of the earlier idea that the residual term was the difference between the observed value of Y and the conditional mean of Y for a given X-value. Mathematically,\n\\[\n\\epsilon_i = Y_i - \\mu_{Y \\vert X_i}\n\\]\nOnce we use data to obtain estimates for the intercept and slope (\\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)) we can substitute those into the fitted equation and obtain an estimate for the conditional mean (\\(\\hat{\\mu}_{Y \\vert X_i}\\)). This value can then be used to obtain an estimate for the residual.\n\\[\n\\hat\\epsilon_i = Y_i - \\hat{\\mu}_{Y \\vert X_i}\n\\]\nOr, using the ‚ÄúY-hat‚Äù notation,\n\\[\n\\hat\\epsilon_i = Y_i - \\hat{Y_i}\n\\]\nRemember, the hat on the residual indicates it is an estimate based on values obtained from the data!"
  },
  {
    "objectID": "02-01-simple-regression-description.html#fitting-a-regression-model-to-data-using-r",
    "href": "02-01-simple-regression-description.html#fitting-a-regression-model-to-data-using-r",
    "title": "6¬† Simple Linear Regression‚ÄîDescription",
    "section": "6.4 Fitting a Regression Model to Data Using R",
    "text": "6.4 Fitting a Regression Model to Data Using R\nTo fit the regression model to data using R, we will use the lm() function. The syntax for this function looks like this:\n\nlm(outcome ~ 1 + predictor, data = dataframe)\n\nwhere outcome is the name of the outcome/response variable, predictor is the name of the predictor variable, and dataframe is the name of the data frame. (The 1 on the right side of the tilde tells R to include the intercept in its computation.) When we fit a regression model in R, we will also assign the output to a new object in R. Below, we fit the model using education level to predict income. Here the output is assigned to an object called lm.a. We can print the regression parameter estimates by typing the lm() object name and hitting enter.\n\n# Fit regression model\nlm.a = lm(income ~ 1 + education, data = city)\n\n# Print regression coefficients\nlm.a\n\n\nCall:\nlm(formula = income ~ 1 + education, data = city)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\n\nHere the parameter estimates (or regression coefficients) are:\n\n\\(\\hat{\\beta}_0 = 11.321\\)\n\\(\\hat{\\beta}_1 = 2.651\\)\n\nRemember that these are estimates and need the hats. The systematic part of the fitted regression model (or fitted equation) is:\n\\[\n\\hat{\\mathrm{Income}_i} = 11.321 + 2.651(\\mathrm{Education~Level}_i)\n\\]\n\n\n6.4.1 Intercept Interpretation\nThe estimate for the intercept was 11.321. Graphically, this value indicates the value where the line passes through the Y-axis (i.e., Y-intercept). As such, it gives the \\(\\hat{Y}_i\\) or predicted value when \\(X=0\\). Algebraically we get the same thing if we substitute 0 in for \\(X_i\\) in the fitted regression equation.\n\\[\n\\begin{split}\n\\hat{Y}_i &= \\hat{\\beta}_0 + \\hat{\\beta}_1(0) \\\\\n\\hat{Y}_i &= \\hat{\\beta}_0\n\\end{split}\n\\]\nTo interpret this value, remember that \\(\\hat{Y}_i\\) is a conditional mean. In this case, it represents the model predicted mean income for all employees that have an education level of 0 years. Graphically this looks like the following.\n\n\n\n\n\nFigure¬†6.1: Plot displaying conditional distribution of Y at \\(X=0\\). The OLS fitted regression line (dotted) is also shown. The red points show the mean value of Y for this conditional distribution‚Äîwhich corresponds to the intercept value of the regression line.\n\n\n\n\nInterpreting the intercept coefficient from our example,\n\nThe model predicted mean income for all employees that have an education level of 0 years is $11,321.\n\n\n\n\n6.4.2 Slope Interpretation\nRecall from algebra that the slope of a line describes the change in Y versus the change in X. In regression, the slope describes the predicted change in \\(\\hat{Y}\\) for a one-unit difference in X. In our example,\n\\[\n\\hat{\\beta}_1 = \\frac{\\Delta\\hat{Y}}{\\Delta X} = \\frac{2.651}{1}\n\\]\nAgain, because \\(\\hat{Y}\\) is a conditional mean, the slope represents the difference in predicted mean incomes for each one-year difference in education level. Graphically,\n\n\n\n\n\nPlot displaying conditional distribution of Y at \\(X=0\\) and \\(X=1\\). The OLS fitted regression line (dotted) is also shown. The red points show the mean value of Y for these conditional distributions‚Äîthe relative change which corresponds to the slope value of the regression line.\n\n\n\n\nInterpreting the slope coefficient in our example,\n\nEach one-year difference in education level is associated with a model predicted difference of $2,651 in mean income.\n\nTo better understand this, consider the mean income for city employees with three different education levels. The first set of employees have an education level of 10 years. The second set has an education level of 11 years, and the third set has an education level of 12 years. Now let‚Äôs compute each set of employees‚Äô mean income using the estimated regression coefficients.\n\\[\n\\begin{split}\n\\mathbf{Education=10:~}\\hat{\\mathrm{Income}} &= 11.321 + 2.651(10) \\\\\n&= 37.831\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\mathbf{Education=11:~}\\hat{\\mathrm{Income}} &= 11.321 + 2.651(11) \\\\\n&= 40.482\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\mathbf{Education=12:~}\\hat{\\mathrm{Income}} &= 11.321 + 2.651(12) \\\\\n&= 43.133\n\\end{split}\n\\]\nEach set of employees‚Äô education levels differ by one year (10 to 11 to 12). The difference in predicted mean incomes for these employees differs by 2.651 thousand dollars.\n\n\n\n6.4.3 Estimating Residuals\nConsider the 25th case in the data frame.\n\ncity |&gt;\n  filter(row_number() == 25)\n\n\n\n  \n\n\n\nThis employee (Employee 25) has an education level of 20 years (\\(X_{25}=20\\)). Her income is 54.672 thousand dollars (\\(Y_{25}=54.672\\)). Using the fitted regression equation, we can compute the model predicted mean income for employees with 20 years of education as,\n\n# Y_hat = b0 + b1 * X\n11.321 + 2.651 * 20\n\n[1] 64.341\n\n\n\\[\n\\hat{Y}_{25} = 64.341\n\\]\nWe could also report this using the conditional mean notation,\n\\[\n\\hat\\mu_{Y \\vert X=20} = 64.341\n\\]\nNow we can use the estimated conditional mean to also compute Employee 25‚Äôs residual.\n\n# e = Y - Y_hat\n54.672 - 64.341\n\n[1] -9.669\n\n\nUsing mathematical notation,\n\\[\n\\hat\\epsilon_{25} = -9.669\n\\]\nThe negative residual, \\(\\hat\\epsilon_{25} = -9.669\\), suggests that this employee earns $9,669 less than the average predicted income for employees with 20 years of education. We can also represent these values graphically.\n\n\n\n\n\nPlot displaying the OLS fitted regression line (blue) between employee education levels and incomes. The 25th employee‚Äôs observed data (black dot) is plotted, and a visual representation of the employee‚Äôs residual (red line) is also displayed."
  },
  {
    "objectID": "02-01-simple-regression-description.html#answering-the-research-question",
    "href": "02-01-simple-regression-description.html#answering-the-research-question",
    "title": "6¬† Simple Linear Regression‚ÄîDescription",
    "section": "6.5 Answering the Research Question",
    "text": "6.5 Answering the Research Question\nRemember that this whole analysis was driven because we wanted to answer a question, namely whether education level is related to income for the Riverview employees. The results from the regression analysis allow us to answer this question.\n\nTo answer the question of whether education level is related to income, a linear regression model was fitted to the data using ordinary least squares estimation. The results of this analysis suggested that education level is positively related to income for the 32 employees (\\(\\hat\\beta_1 = 2.65\\)). Each year of education is associated with a $2,651 difference in income, on average.\n\nHere the regression analysis provides a quantitative summary of the relationship between education level and income. It provides us with information about the direction of the relationship (positive) and the magnitude of that relationship. Although this can give us a description of the relationship, it is only for the sample of data you looked at (i.e., for these 32 employees). To make further statements about whether there is a relationship between education level and income in a broader population (e.g., all Riverview employees, or all California residents), we need more information, namely whether the sample is representative of the larger population and also statistical information about the amount of sampling error we have. (We will cover sampling error in Chapter 5.)"
  },
  {
    "objectID": "02-01-simple-regression-description.html#footnotes",
    "href": "02-01-simple-regression-description.html#footnotes",
    "title": "6¬† Simple Linear Regression‚ÄîDescription",
    "section": "",
    "text": "Technically there are many unknown residuals, one for each case, but the assumptions we put on the linear model make it so that we only care about the variance of the residuals, hence a single unknown.‚Ü©Ô∏é\nSometimes people use Roman letters when referring to sample estimates rather than hatting the Greek letters. For example, the sample-based equation might be denoted: \\(\\hat{Y}_i = b_0 + b_1(X_i) + e_i\\).‚Ü©Ô∏é"
  },
  {
    "objectID": "02-02-ols-estimation.html#ordinary-least-squares-estimation",
    "href": "02-02-ols-estimation.html#ordinary-least-squares-estimation",
    "title": "7¬† Ordinary Least Squares (OLS) Estimation",
    "section": "7.1 Ordinary Least Squares Estimation",
    "text": "7.1 Ordinary Least Squares Estimation\nHow does R determine the coefficient values of \\(\\hat{\\beta}_0=11.321\\) and \\(\\hat{\\beta}_1=2.651\\)? These values are estimated from the data using a method called Ordinary Least Squares (OLS). To understand how OLS works, consider the following toy data set of five observations:\n\n\n\n\n\n  \n  Table¬†7.1:  Toy data set with predictor (X) and outcome (Y) for five\nobservations. \n  \n    \n    \n      Xi\n      Yi\n    \n  \n  \n    30\n63\n    10\n44\n    30\n40\n    50\n68\n    20\n25\n  \n  \n  \n\n\n\n\n\nWhich of the following two models fits these data better?\n\nModel A: \\(~~\\hat{Y_i} = 28 + 0.8(X_i)\\)\nModel B: \\(~~\\hat{Y_i} = 20 + 1(X_i)\\)\n\nWe could plot the data and both lines and try to determine which seems to fit better.\n\n\n\n\n\n\n\nScatterplot of the observed toy data and the OLS fitted regression line for Model A.\n\n\n\n\n\n\n\n\n\nScatterplot of the observed toy data and the OLS fitted regression line for Model B."
  },
  {
    "objectID": "02-02-ols-estimation.html#datamodel-fit",
    "href": "02-02-ols-estimation.html#datamodel-fit",
    "title": "7¬† Ordinary Least Squares (OLS) Estimation",
    "section": "7.2 Data‚ÄìModel Fit",
    "text": "7.2 Data‚ÄìModel Fit\nIn this case, the lines are similar and it is difficult to make a determination of which fits the data better by eyeballing the two plots. Instead of guessing which model fits better, we can actually quantify the fit for the data by computing the residuals (errors) for each model and then comparing both sets of residuals; larger errors indicate a worse fitting model (i.e., more misfit to the data).\nRemember, to compute the residuals, we will first need to compute the predicted value (\\(\\hat{Y}_i\\)) for each of the five observations for both models.\n\n\n\n\n\n  \n  Table¬†7.2:  Observed values, predicted values and residuals for Model A. \n  \n    \n    \n      Xi\n      Yi\n      YÃÇi\n      ŒµÃÇi\n    \n  \n  \n    30\n63\n52\n11\n    10\n44\n36\n8\n    30\n40\n52\n-12\n    50\n68\n68\n0\n    20\n25\n44\n-19\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n  \n  Table¬†7.3:  Observed values, predicted values and residuals for Model B. \n  \n    \n    \n      Xi\n      Yi\n      YÃÇi\n      ŒµÃÇi\n    \n  \n  \n    30\n63\n50\n13\n    10\n44\n30\n14\n    30\n40\n50\n-10\n    50\n68\n70\n-2\n    20\n25\n40\n-15\n  \n  \n  \n\n\n\n\n\nEyeballing the numeric values of the residuals is also problematic. The size of the residuals is similar for both Models. Also, the eyeballing method would be impractical for larger datasets. So, we have to further quantify the model fit (or misfit). The way we do that in practice is to consider the total amount of error across all the observations. Unfortunately, we cannot just sum the residuals to get the total because some of our residuals are negative and some are positive. To alleviate this problem, we first square the residuals, then we sum them.\n\\[\n\\begin{split}\n\\mathrm{Total~Error} &= \\sum\\hat{\\epsilon}_i^2 \\\\\n&= \\sum \\left( Y_i - \\hat{Y}_i\\right)^2\n\\end{split}\n\\]\nThis is called a sum of squared residuals or sum of squared error (SSE; good name, isn‚Äôt it). Computing the squared residuals for Model A and Model B we get:\n\n\n\n\n\n  \n  Table¬†7.4:  Observed values, predicted values, residuals, and squared residuals\nfor Model A. \n  \n    \n    \n      Xi\n      Yi\n      YÃÇi\n      ŒµÃÇi\n      ŒµÃÇi2\n    \n  \n  \n    30\n63\n52\n11\n121\n    10\n44\n36\n8\n64\n    30\n40\n52\n-12\n144\n    50\n68\n68\n0\n0\n    20\n25\n44\n-19\n361\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n  \n  Table¬†7.5:  Observed values, predicted values, residuals, and squared residuals\nfor Model B. \n  \n    \n    \n      Xi\n      Yi\n      YÃÇi\n      ŒµÃÇi\n      ŒµÃÇi2\n    \n  \n  \n    30\n63\n50\n13\n169\n    10\n44\n30\n14\n196\n    30\n40\n50\n-10\n100\n    50\n68\n70\n-2\n4\n    20\n25\n40\n-15\n225\n  \n  \n  \n\n\n\n\n\nSumming these squared values for each model we obtain:\n\nModel A: SSE = 690\nModel B: SSE = 694\n\nOnce we have quantified the model misfit, we can choose the model that has the least amount of error. Since Model A has a lower SSE than Model B, we would conclude that Model A is the better fitting model to the data.\n\n\n7.2.1 Visualizing the SSE\nTo further understand the sum of squared error, we can examine a visual representation of the SSE for Model A. Recall that visually, the residual is the vertical distance between an observation and the fitted value (which lie on the fitted line). The residual indicates how different these two quantities are on the Y-metric. In the formula we squared each of the residuals. Visually, this is equivalent to producing the area of a square that has a side length equal to the absolute value of the residual.\n\n\n\n\n\n\n\n\nThis plot visually displays the residual values as line segments with negative residuals shown as dashed lines.\n\n\n\n\n\n\n\n\n\nThis plot visually displays the squared residuals as the area of a square with side length equal to the absolute value of the residual.\n\n\n\n\n\nFigure¬†7.1: Scatterplot of the observed toy data and the OLS fitted regression line for Model A.\n\n\nThe SSE is simply the total area encompassed by all of the squares. Note that the observation that is directly on the line has a residual of 0 and thus does not contribute a quantity to the SSE. If you computed the SSE for a line with different intercept or slope values, the SSE will be different. The plot below shows what this might look like for the flat line produced by \\(~~\\hat{Y_i} = 50\\).\n\n\n\n\n\nScatterplot of the observed toy data and the fitted flat line with Y-intercept of 50. The plot visually shows the squared residuals as the area of a square with side length equal to the absolute value of the residual.\n\n\n\n\nPowell & Lehe (2015) created an interactive website to help understand how the SSE is impacted by changing the intercept or slope of a line. You can also see how individual observations impact the SSE value."
  },
  {
    "objectID": "02-02-ols-estimation.html#best-fitting-model",
    "href": "02-02-ols-estimation.html#best-fitting-model",
    "title": "7¬† Ordinary Least Squares (OLS) Estimation",
    "section": "7.3 ‚ÄúBest‚Äù Fitting Model",
    "text": "7.3 ‚ÄúBest‚Äù Fitting Model\nIn the vocabulary of statistical estimation, the process we just used to adopt Model A over Model B was composed of two parts:\n\nQuantification of Model Fit: We quantify how well (or not well) the estimated model fits the data; and\nOptimization: We find the ‚Äúbest‚Äù model based on that quantification. (This boils down to finding the model that produces the biggest or smallest measure of model fit.)\n\nIn our example we used the SSE as the quantification of model fit, and then we optimized by selecting the model with the lower SSE. When we use lm() to fit a regression analysis to the data, R needs to consider not just two models like we did in our example, but all potential models (i.e., any intercept and slope). The model coefficients that lm() returns are the ‚Äúbest‚Äù in that no other values for intercept or slope would produce a lower SSE. The model returned has the lowest SSE possible thus least squares. For our toy dataset, the model that produces the smallest residuals is\n\\[\n\\hat{Y}_i = 28.682 + 8.614(X_i)\n\\]\nThis model gives the following predicted values and residuals:\n\n\n\n\n\n  \n  Table¬†7.6:  Observed values, predicted values, residuals, and squared residuals\nfor the ‚Äòbest‚Äô fitting model. \n  \n    \n    \n      Xi\n      Yi\n      YÃÇi\n      ŒµÃÇi\n      ŒµÃÇi2\n    \n  \n  \n    30\n63\n49.61364\n13.386364\n179.1947\n    10\n44\n33.47727\n10.522727\n110.7278\n    30\n40\n49.61364\n-9.613636\n92.4220\n    50\n68\n65.75000\n2.250000\n5.0625\n    20\n25\n41.54545\n-16.545455\n273.7521\n  \n  \n  \n\n\n\n\n\nThe SSE is 661.16. This is the smallest SSE possible for a linear model. Any other value for the slope or intercept would result in a higher SSE.\n\n\n7.3.1 Mathematical Optimization\nFinding the intercept and slope that give the lowest SSE is referred to as an optimization problem in the field of mathematics. Optimization is such an important (and sometimes difficult) problem that there have been several mathematical and computational optimization methods that have been developed over the years. You can read more about mathematical optimization on Wikipedia if you are interested.\nOne common mathematical method to find the minimum SSE involves calculus. We would write the SSE as a function of\\(\\beta_0\\) and \\(\\beta_1\\), compute the partial derivatives (w.r.t. each of the coefficients), set these equal to zero, and solve to find the values of the coefficients.  The lm() function actually uses an optimization method called QR decomposition to obtain the regression coefficients. The actual mechanics and computation of these methods are beyond the scope of this course. We will just trust that the lm() function is doing things correctly in this course."
  },
  {
    "objectID": "02-02-ols-estimation.html#computing-the-sse-for-the-model-fitted-to-the-riverview-data",
    "href": "02-02-ols-estimation.html#computing-the-sse-for-the-model-fitted-to-the-riverview-data",
    "title": "7¬† Ordinary Least Squares (OLS) Estimation",
    "section": "7.4 Computing the SSE for the Model Fitted to the Riverview Data",
    "text": "7.4 Computing the SSE for the Model Fitted to the Riverview Data\nSince the regression model is based on the lowest SSE, it is often useful to compute and report the model‚Äôs SSE. We can use R to compute the SSE by carrying out the computations underlying the formula for SSE. Recall that the SSE is\n\\[\n\\mathrm{SSE} = \\sum \\left( Y_i - \\hat{Y}_i\\right)^2\n\\]\nWe need to compute the:\n\nPredicted values (\\(\\hat{Y}_i\\));\nResiduals (\\(e_i\\));\nSquared residuals (\\(e_i^2\\)); and finally,\nSum of the squared residuals (\\(\\sum e_i^2\\)).\n\nFrom the Riverview data set we have the observed X (education level) and Y (income) values, and from the fitted lm() we have the intercept and slope estimates for the ‚Äòbest‚Äô fitting regression model.\n\n# Step 1: Compute the predicted values of Y\ncity |&gt;\n  mutate(\n    y_hat = 11.321 + 2.651 * education\n    )\n\n\n\n  \n\n\n# Step 2: Compute the residuals\ncity |&gt;\n  mutate(\n    y_hat = 11.321 + 2.651 * education,\n    errors = income - y_hat\n    )\n\n\n\n  \n\n\n# Step 3: Compute the squared residuals\ncity |&gt;\n  mutate(\n    y_hat = 11.321 + 2.651 * education,\n    errors = income - y_hat,\n    sq_errors = errors ^ 2\n  )\n\n\n\n  \n\n\n# Step 4: Compute the sum of the squared residuals\ncity |&gt;\n  mutate(\n    y_hat = 11.321 + 2.651 * education,\n    errors = income - y_hat,\n    sq_errors = errors ^ 2\n  ) |&gt;\n  summarize(\n    SSE = sum(sq_errors)\n  )\n\n\n\n  \n\n\n\nThe SSE gives us information about the variation in Y (the outcome variable) that is left over (residual) after we fit the regression model. Since the regression model is a function of X, the SSE tells us about the variation in Y that is left over after we remove the variation associated with, or accounted for by X. In our example it tells us about the residual variation in incomes after we account for employee education level.\nIn practice, we often report the SSE, but we do not interpret the actual value. The value of the SSE is more useful when comparing models. When researchers are considering different models, the SSEs from these models are compared to determine which model produces the least amount of misfit to the data (similar to what we did earlier).\n\n\n7.4.1 Evaluating the Impact of a Predictor Using SSE\nConsider again the general equation for the statistical model that includes a single predictor,\n\\[\nY_i = \\beta_0 + \\beta_1(X_i) + \\epsilon_i\n\\]\nOne way that statisticians evaluate a predictor is to compare a model that includes that predictor to the same model that does not include that predictor. For example, comparing the following two models allows us to evaluate the impact of \\(X_i\\).\n\\[\n\\begin{split}\nY_i &= \\beta_0 + \\beta_1(X_i) + \\epsilon_i \\\\\nY_i &= \\beta_0 + \\epsilon_i\n\\end{split}\n\\]\nThe second model, without the effect of X, is referred to as the intercept-only model. This model implies that the value of Y is not a function of X. In our example it suggests that the mean income is not conditional on education level. The fitted equation,\n\\[\n\\hat{Y}_i = \\hat{\\beta}_0\n\\]\nindicates that the predicted Y would be the same (constant) regardless of what X is. In our example, this would be equivalent to saying that the mean income is the same, regardless of employee education level.\n\n\n\n7.4.2 Fitting the Intercept-Only Model\nTo fit the intercept-only model, we just omit the predictor term on the right-hand side of the lm() formula.\n\nlm.0 = lm(income ~ 1, data = city)\nlm.0\n\n\nCall:\nlm(formula = income ~ 1, data = city)\n\nCoefficients:\n(Intercept)  \n      53.74  \n\n\nThe fitted regression equation for the intercept-only model can be written as,\n\\[\n\\hat{\\mathrm{Income}_i} = 53.742\n\\]\nGraphically, the fitted line is a flat line crossing the \\(y\\)-axis at 53.742 (see plot below).\n\n\n\n\n\nScatterplot of employee incomes versus education levels. The OLS fitted regression line for the intercept-only model is also displayed.\n\n\n\n\nDoes the estimate for \\(\\beta_0\\), 53.742, seem familiar? If not, go back to the exploration of the response variable in the Simple Linear Regression‚ÄîDescription chapter. The estimated intercept in the intercept-only model is the marginal mean value of the response variable. This is not a coincidence.\nRemember that the regression model estimates the mean. Here, since the model is not a conditional model (no X predictor) the expected value (mean) is the marginal mean.\n\\[\n\\begin{split}\n\\mu_Y &= \\beta_0 \\\\\n\\end{split}\n\\]\nPlotting this we get,\n\n\n\n\n\nPlot displaying the OLS fitted regression line for the intercept-only model. Histogram showing the marginal distributon of incomes is also shown.\n\n\n\n\nThe model itself does not consider any predictors, so on the plot, the X variable is superfluous; we could just collapse it to its margin. This is why the mean of all the Y values is sometimes referred to as the marginal mean.\nYet another way to think about this is that the model is choosing a single income (\\(\\hat{\\beta}_0\\)) to be the predicted income for all the employees. Which value would be a good choice? Remember the lm() function chooses the ‚Äúbest‚Äù value for the parameter estimate based on minimizing the sum of squared errors. The marginal mean is the value that minimizes the squared deviations (errors) across all of the observations, regardless of education level. This is one reason the mean is often used as a summary measure of a set of data.\n\n\n\n7.4.3 SSE for the Intercept-Only Model\nSince the intercept-only model does not include any predictors, the SSE for this model is a quantification of the total variation in the outcome variable. It can be used as a baseline measure of the error variation in the data. Below we compute the SSE for the intercept-only model (if you need to go through the steps one-at-a-time, do so.)\n\ncity |&gt;\n  mutate(\n    y_hat = 53.742,\n    errors = income - y_hat,\n    sq_errors = errors ^ 2\n  ) |&gt;\n  summarize(\n    SSE = sum(sq_errors)\n  )\n\n\n\n  \n\n\n\n\n\n\n7.4.4 Proportion Reduction in Error\nThe SSE for the intercept-only model represents the total amount of variation in the sample incomes. As such we can use it as a baseline for comparing other models that include predictors. For example,\n\nSSE (Intercept-Only): 6566\nSSE (w/Education Level Predictor): 2418\n\nOnce we account for education in the model, we reduce the SSE. Moreover, since the only difference between the intercept-only model and the predictor model was the inclusion of the effect of education level, any difference in the SSE is attributable to including education in the model. Since the SSE is smaller after we include education level in the model it implies that improving the data‚Äìmodel fit (smaller error).\nHow much did the amount of error improve? The SSE was reduced by 4148 after including education level in the model. Is this a lot? To answer that question, we typically compute and report this reduction as a proportion of the total variation; called the proportion of the reduction in error, or PRE.\n\\[\n\\mathrm{PRE} = \\frac{\\mathrm{SSE}_{\\mathrm{Intercept\\mbox{-}Only}} - \\mathrm{SSE}_{\\mathrm{Predictor\\mbox{-}Model}}}{\\mathrm{SSE}_{\\mathrm{Intercept\\mbox{-}Only}}}\n\\]\nFor our particular example,\n\\[\n\\begin{split}\n\\mathrm{PRE} &= \\frac{6566 - 2418}{6566} \\\\\n&= \\frac{4148}{6566} \\\\\n&= 0.632\n\\end{split}\n\\]\nIncluding education level as a predictor in the model reduced the error by 63.2%."
  },
  {
    "objectID": "02-02-ols-estimation.html#partitioning-variation",
    "href": "02-02-ols-estimation.html#partitioning-variation",
    "title": "7¬† Ordinary Least Squares (OLS) Estimation",
    "section": "7.5 Partitioning Variation",
    "text": "7.5 Partitioning Variation\nUsing the SSE terms we can partition the total variation in \\(Y\\) (the SSE value from the intercept-only model) into two parts: (1) the part that is explained by the model, and (2) the part that remains unexplained. The unexplained variation is just the SSE from the regression model that includes \\(X\\); remember it is residual variation. Here is the partitioning of the variation in income.\n\\[\n\\underbrace{6566}_{\\substack{\\text{Total} \\\\ \\text{Variation}}} = \\underbrace{4148}_{\\substack{\\text{Explained} \\\\ \\text{Variation}}} + \\underbrace{2418}_{\\substack{\\text{Unexplained} \\\\ \\text{Variation}}}\n\\]\nEach of these three terms is a sum of squares (SS). The first is referred to as the total sum of squares, as it represents the total amount of variation in \\(Y\\). The second term is commmonly called the model sum of squares (or, regression sum of squares), as it represents the variation explained by the model. The last term is the error sum of squares (or, residual sum of squares) as it represents the left-over variation that is unexplained by the model.\nMore generally,\n\\[\n\\mathrm{SS_{\\mathrm{Total}}} = \\mathrm{SS_{\\mathrm{Model}}} + \\mathrm{SS_{\\mathrm{Error}}}\n\\]\n\n\n7.5.1 Variation Accounted For\nIt is often convenient to express these values as proportions of the total variation. To do this we can divide each term in the partitioning by the total sum of squares.\n\\[\n\\frac{\\mathrm{SS_{\\mathrm{Total}}}}{\\mathrm{SS_{\\mathrm{Total}}}} = \\frac{\\mathrm{SS_{\\mathrm{Model}}}}{\\mathrm{SS_{\\mathrm{Total}}}} + \\frac{\\mathrm{SS_{\\mathrm{Error}}}}{\\mathrm{SS_{\\mathrm{Total}}}}\n\\]\nUsing the values from our example,\n\\[\n\\begin{split}\n\\frac{6566}{6566} &= \\frac{4148}{6566} + \\frac{2418}{6566} \\\\[2ex]\n1 &= 0.632 + 0.368\n\\end{split}\n\\]\nThe first term on the right-hand side of the equation, \\(\\frac{\\mathrm{SS_{\\mathrm{Model}}}}{\\mathrm{SS_{\\mathrm{Total}}}}\\), is 0.632. This is the PRE value we computed earlier. Since the \\(\\mathrm{SS_{\\mathrm{Model}}}\\) represents the model-explained variation, many researchers interpret this value as the percentage of variation explained or accounted for by the model. They might say,\n\nThe model accounts for 63.2% of the variation in incomes.\n\nSince the only predictor in the model is education level, an alternative interpretation of this value is,\n\nDifferences in education level account for 63.2% of the variation in incomes.\n\nBetter models explain more variation in the outcome. They also have small errors. Aside from conceptually making some sense, this is also shown in the mathematics of the partitioning of variation.\n\\[\n1 = \\frac{\\mathrm{SS_{\\mathrm{Model}}}}{\\mathrm{SS_{\\mathrm{Total}}}} + \\frac{\\mathrm{SS_{\\mathrm{Error}}}}{\\mathrm{SS_{\\mathrm{Total}}}}\n\\]\nSince the denominator is the same on both terms, and the sum of the two terms must be one, this implies that the smaller the amount of error, the smaller the last term (proportion of unexplained variation ) must be and the larger the first term (the proportion of explained variation) has to be.\n\n\n\n7.5.2 R-Squared\nAnother way to think about measuring the quality of a model is that ‚Äògood‚Äô models should reproduce the observed outcomes, after all they explain variation in the outcome. How well do the fitted (predicted) values from our model match wih the outcome values? To find out, we can compute the correlation between the model fitted values and the observed outcome values. To compute a correlation, we will use the correlate() function from the {corrr} package.\n\n# Load library\nlibrary(corrr)\n\n# Create fitted values and correlate them with the outcome\ncity |&gt;\n  mutate(\n    y_hat = 11.321 + 2.651*education\n  ) |&gt;\n  select(y_hat, income) |&gt;\n  correlate()\n\n\n\n  \n\n\n\nThe correlation between the observed and fitted values is 0.795. This is a high correlation indicating that the model fitted values and the observed values are similar. We denote this value using the uppercase Roman letter \\(R\\).\n\\[\nR = 0.795\n\\]\nNow square this value.\n\\[\nR^2 = 0.795^2 = 0.632\n\\]\nAgain we get the PRE value! All three ways of expressing this metric of model quality are equivalent:\n\n\\(\\frac{\\mathrm{SSE}_{\\mathrm{Intercept\\mbox{-}Only}} - \\mathrm{SSE}_{\\mathrm{Predictor\\mbox{-}Model}}}{\\mathrm{SSE}_{\\mathrm{Intercept\\mbox{-}Only}}}\\)\n\\(\\frac{\\mathrm{SS_{\\mathrm{Model}}}}{\\mathrm{SS_{\\mathrm{Total}}}}\\)\n\\(R^2\\)\n\nAlthough these indices seem to measure different aspects of model quality‚Äîreduction in error variation, model explained variation, and alignment of the model fitted and observed values‚Äîwith OLS fitted linear models, these values are all equal. This will not necessarily be true when we estimate model parameters using a different estimation method (e.g., maximum likelihood). Most of the time this value will be reported in applied research as \\(R^2\\), but as you can see, there are many interpretations of this value under the OLS framework.\n\n\n\n7.5.3 Back to Partitioning\nUsing the fact that \\(R^2 = \\frac{\\mathrm{SS_{\\mathrm{Model}}}}{\\mathrm{SS_{\\mathrm{Total}}}}\\), we can substitute this into the partitioning equation from earlier.\n\\[\n\\begin{split}\n\\frac{\\mathrm{SS_{\\mathrm{Total}}}}{\\mathrm{SS_{\\mathrm{Total}}}} &= \\frac{\\mathrm{SS_{\\mathrm{Model}}}}{\\mathrm{SS_{\\mathrm{Total}}}} + \\frac{\\mathrm{SS_{\\mathrm{Error}}}}{\\mathrm{SS_{\\mathrm{Total}}}} \\\\[2ex]\n1 &= R^2 + \\frac{\\mathrm{SS_{\\mathrm{Error}}}}{\\mathrm{SS_{\\mathrm{Total}}}} \\\\[2ex]\n1 - R^2 &= \\frac{\\mathrm{SS_{\\mathrm{Error}}}}{\\mathrm{SS_{\\mathrm{Total}}}}\n\\end{split}\n\\]\nThis suggests that the last term in the partititoning, \\(\\frac{\\mathrm{SS_{\\mathrm{Error}}}}{\\mathrm{SS_{\\mathrm{Total}}}}\\) is simply the difference between 1 and \\(R^2\\). In our example,\n\n\\(R^2 = 0.632\\), and\n\\(1 - R^2 = 0.368\\)\n\nRemember that one interpretation of \\(R^2\\) is that 63.2% of the variation in incomes was explained by the model. Alternatively, 36.8% of the variation in income is not explained by the model; it is residual variation. If the unexplained variation is too large, it suggests to an applied analyst that she could include additional predictors in the model. We will explore this in future chapters.\nFor now, recognize that OLS estimation gives us the ‚Äòbest‚Äô model in terms of minimizing the sum of squared residuals, which in turn maximizes the explained variation. But, the ‚Äòbest‚Äô model may not be a ‚Äògood‚Äô model. One way to measure the quality of the model is through the metric of \\(R^2\\). Understanding whether \\(R^2\\) is large or small is based on the domain science. For example in some areas of educational research, an \\(R^2\\) of 0.4 might indicate a really great model, whereas the same \\(R^2\\) of 0.4 in some areas of biological research might be quite small and indicate a poor model."
  },
  {
    "objectID": "02-02-ols-estimation.html#references",
    "href": "02-02-ols-estimation.html#references",
    "title": "7¬† Ordinary Least Squares (OLS) Estimation",
    "section": "7.6 References",
    "text": "7.6 References\n\n\n\n\nPowell, V., & Lehe, L. (2015). Ordinary least squares regression: Explained visually. Explained Visually: A Setosa Project. https://setosa.io/ev/ordinary-least-squares-regression/"
  },
  {
    "objectID": "02-03-correlation.html#data-exploration",
    "href": "02-03-correlation.html#data-exploration",
    "title": "8¬† Correlation and Standardized Regression",
    "section": "8.1 Data Exploration",
    "text": "8.1 Data Exploration\nWe begin by looking at the marginal distributions of both time spent on homework and GPA. We will also examine summary statistics of these variables (output presented in ?tbl-02-02-1). Finally, we also examine a scatterplot of GPA versus time spent on homework. (Note that the syntax is not shown.)\n\n\n\n\n\n\n\n\n\nFigure¬†8.1: Density plots of the marginal distribution of GPA.\n\n\n\n\n\n\n\n\n\nFigure¬†8.2: Density plots of the marginal distribution of time spent on homework.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†8.3: Scatterplot showing the relationship between GPA and time spent on homework.\n\n\n\n\n\n\n\n\n\n\n\n  \n  Table¬†8.1:  Summary measures for 8th-Grade students‚Äô GPA and time spent on\nhomework \n  \n    \n    \n      Measure\n      M\n      SD\n    \n  \n  \n    GPA\n80.47\n7.62\n    Time spent on homework\n5.09\n2.06\n  \n  \n  \n\n\n\n\n\nWe might describe the results of this analysis as follows:\n\nThe marginal distributions of GPA and time spent on homework are both unimodal. The average amount of time these 8th-grade students spend on homework each week is 5.09 hours (SD = 2.06). These 8th-grade students have a mean GPA of 80.47 (SD = 7.62) on a 100-pt scale. There is a moderate, positive, linear relationship between time spent on homework and GPA for these students. This suggests that 8th-grade students who spend less time on homework tend to have lower GPAs, on average, than students who spend more time on homework."
  },
  {
    "objectID": "02-03-correlation.html#correlation",
    "href": "02-03-correlation.html#correlation",
    "title": "8¬† Correlation and Standardized Regression",
    "section": "8.2 Correlation",
    "text": "8.2 Correlation\nTo numerically summarize the linear relationship between variables, we typically compute correlation coefficients. The correlation coefficient is a quantification of the direction and strength of the relationship. (It is important to note that the correlation coefficient is only an appropriate summarization of the relationship if the functional form of the relationship is linear.)\nTo compute the correlation coefficient, we use the correlate() function from the {corrr} package. We can use the dplyr-type syntax to select the variables we want correlations between, and then pipe that into the correlate() function. Typically the response (or outcome) variable is the first variable provided in the select() function, followed by the predictor.\n\n# Compute correlation between GPA and time spent on HW\nkeith |&gt;\n  select(gpa, homework) |&gt;\n  correlate()\n\n\n\n  \n\n\n\nWhen reporting the correlation coefficient between variables it is conventional to use a lower-case r and report the value to two decimal places.1 Subscripts are also generally used to indicate the variables. For example,\n\\[\nr_{\\mathrm{GPA,~Homework}} = 0.33\n\\]\nIt is important to keep in mind this value is only useful as a measure of the strength of the relationship when the relationship between variables is linear. Here is an example where the correlation coefficient would be misleading about the strength of the relationship.\n\n\n\n\n\nFigure¬†8.4: Hours of daylight versus day of the year for \\(n=75\\) days in Minneapolis.\n\n\n\n\nHere there is a perfect relationship between day of the year and hours of daylight. If you fitted a nonlinear model here, your ‚Äúline‚Äù would match the data exactly (no residual error!). But the correlation coefficient does not reflect that (\\(r=-0.34\\)).\n\nYou should always create a scatterplot to examine the relationship graphically before computing a correlation coefficient to numerically summarize it.\n\nAnother situation in which correlation can mislead is when you have subpopulations in your data. Here is an example of that.\n\n\n\n\n\n\n\nFigure¬†8.5: Salary versus neuroticism (0 = not at all neurotic; 7= very neurotic) as measured by the Big Five personality survey for \\(n=1000\\) employees from a Fortune 500 company.\n\n\n\n\n\n\n\n\n\nFigure¬†8.6: Salary versus neuroticism (0 = not at all neurotic; 7= very neurotic) as measured by the Big Five personality survey for \\(n=1000\\) employees from a Fortune 500 company. The data are colored by education level.\n\n\n\n\n\n\nIf we treat these data as one population (an assumption for using the correlation) the relationship between neurotocism and salary is positive; employees who are more neurotic tend to have higher salaries, on average. However, if we account for education level, the relationship between neurotocism and salary is negative for each of the education levels; once we account for education level, employees who are more neurotic tend to have lower salaries, on average. This reversal of the direction of the relationship once we account for other variables is quite common (so common it has a name, Simpson‚Äôs Paradox) and makes it difficult to be sure about the ‚Äútrue‚Äù relationship between variables in observational data."
  },
  {
    "objectID": "02-03-correlation.html#understanding-correlation",
    "href": "02-03-correlation.html#understanding-correlation",
    "title": "8¬† Correlation and Standardized Regression",
    "section": "8.3 Understanding Correlation",
    "text": "8.3 Understanding Correlation\nThere are many equivalent computational formulas for calculating the correlation coefficient. Each of these were useful in the days when we needed to hand-calculate the correlation. In practice, we now just use the computer to calculate the value of the correlation coefficient. That being said, some of these formulas are useful in helping us better understand what the correlation coefficient is measuring. Below is one such expression:\n\\[\nr_{xy} = \\frac{1}{n-1}\\sum_{i=1}^n\\bigg(\\frac{x_i - \\bar{x}}{s_x}\\bigg)\\bigg(\\frac{y_i - \\bar{y}}{s_y}\\bigg)\n\\]\nwhere, n is the sample size; \\(x_i\\) and \\(y_i\\) are the values for observation i of the variables x and y, respectively; \\(\\bar{x}\\) and \\(\\bar{y}\\) are the mean values for the variables x and y, respectively; and \\(s_x\\) and \\(s_y\\) are the standard deviations for the variables x and y, respectively.\nNote that the terms in the parentheses are the z-scores for the x- and y-values for a particular observation. Thus, this formula can be re-written as:\n\\[\nr_{xy} = \\frac{1}{n-1}\\sum_{i=1}^n\\bigg(z_{xi}\\bigg)\\bigg(z_{yi}\\bigg)\n\\]\nThis formula essentially says, multiply the z-scores of x and y together for each observation; add them together, and divide by the sample size.2 Adding things together and dividing by the sample size is the way we calculate an average. The correlation coefficient is an average of sorts! It is essentially the average product of the z-scores.\nAs we consider the product of the z-scores for x and y, recall that the z-score gives us information about how many standard deviations an observation is from the mean. Moreover, it gives us information about whether the observation is above the mean (positive z-score) or below the mean (negative z-score). Consider an observation that has both an x-value and y-value above the mean. That observation‚Äôs product would be positive.\n\\[\n\\begin{split}\n\\mathrm{\\scriptsize +} z_{x_i} &\\times \\mathrm{\\scriptsize +} z_{y_i} \\\\\n\\mathrm{positive~number} &\\times \\mathrm{positive~number}\n\\end{split}\n\\]\nThis would also be true for an observation that has both an x-value and y-value below the mean.\n\\[\n\\begin{split}\n-z_{x_i} &\\times -z_{y_i} \\\\\n\\mathrm{negative~number} &\\times \\mathrm{negative~number}\n\\end{split}\n\\]\nObservations the are above the mean on one variable and below the mean on the other would have a negative product. Here is a plot of the standardized GPA versus the standardized time spent on homework for the 100 observations. The mean values are also displayed.\n\n\n\n\n\nFigure¬†8.7: Plot of the standardized GPA versus the standardized time spent on homework for the 100 observations. The mean values are also displayed.\n\n\n\n\nIn this case there more observations having a positive product of z-scores than a negative product of z-scores. This suggests that the sum of all of these products will be positive; the correlation coefficient will be positive.3\nConceptually, that sum of products of z-scores in the formula for the correlation coefficient gives us an indication of the patterns of deviation from the mean values of x and y for the propensity of the data. The division by \\(n-1\\) serves to give us an indication of the magnitude of the ‚Äúaverage‚Äù product. This is why we interpret positive and negative relationships the way we do; a positive relationship suggests that higher values of x are typically associated with higher values of y and that lower values of x are typically associated with lower values of y. (Note that the words ‚Äúhigher‚Äù and ‚Äúlower‚Äù in that interpretation could more accurately be replaced with ‚Äúvalues above the mean‚Äù and ‚Äúvalues below the mean‚Äù, respectively.)\nWhen we say the direction of the relationship is positive, we statistically mean that the average product of z-scores is positive, which means that the propensity of the data has values of both variables either above or below the mean.\nOf course, we don‚Äôt have to use z-scores to see this pattern, afterall we typically look at a scatterplot of the unstandardized values to make this interpretation.\n\n\n\n\n\nFigure¬†8.8: Plot of GPA versus the time spent on homework (both unstandardized) for the 100 observations. The mean values are also displayed.\n\n\n\n\nConverting to z-scores is only useful to remove the metrics from the unstandardized values and place them on a common scale. This way values of the correlation coefficient are not dependent on the scales used in the data. This is why we do not put a metric on the correlation coefficient (e.g., it is just 0.30, not 0.30 feet)."
  },
  {
    "objectID": "02-03-correlation.html#correlations-relationship-to-regression",
    "href": "02-03-correlation.html#correlations-relationship-to-regression",
    "title": "8¬† Correlation and Standardized Regression",
    "section": "8.4 Correlation‚Äôs Relationship to Regression",
    "text": "8.4 Correlation‚Äôs Relationship to Regression\nThe correlation coefficient and the slope of the regression line are directly related to one another. Mathematically, the estimated slope of the simple regression line can be computed as:\n\\[\n\\hat\\beta_1 = r_{xy} \\times \\frac{s_y}{s_x}\n\\]\nwhere, \\(s_x\\) and \\(s_y\\) are the standard deviations for the variables \\(x\\) and \\(y\\), respectively, and \\(r_{xy}\\) is the correlation between \\(x\\) and \\(y\\). If we are carrying out a regression analysis, there must be variation in both \\(x\\) and \\(y\\), which implies that both \\(s_x\\) and \\(s_y\\) are greater than 0. This in turn implies that the ratio of the standard deviations (the second term on the right-hand side of the equation) is also a positive number. This means the sign of the slope is completely dependent on the sign of the correlation coefficient. If \\(r_{xy}&gt;0\\) then \\(\\hat\\beta_1&gt;0\\). If \\(r_{xy}&lt;0\\) then \\(\\hat\\beta_1&lt;0\\).\nThe magnitude of the regression slope (sometimes referred to as the effect of \\(x\\) on \\(y\\)) is impacted by three factors: the magnitude of the correlation between \\(x\\) and \\(y\\), the amount of variation in \\(y\\), and the amount of variation in \\(x\\). In general, there is a larger effect of \\(x\\) on \\(y\\) when:\n\nThere is a stronger relationship (higher correlation; positive or negative) between x and y;\nThere is more variation in the outcome; or\nThere is less variation in the predictor."
  },
  {
    "objectID": "02-03-correlation.html#standardized-regression",
    "href": "02-03-correlation.html#standardized-regression",
    "title": "8¬† Correlation and Standardized Regression",
    "section": "8.5 Standardized Regression",
    "text": "8.5 Standardized Regression\nIn standardized regression, the correlation plays a more obvious role. Standardized regression is simply regression performed on the standardized variables (z-scores) rather than on the unstandardized variables. To carry out a standardized regression:\n\nStandardize the outcome and predictor(s)\nFit a model by regressing \\(z_y\\) on \\(z_x\\)\n\nHere we will perform a standardized regression on the Keith data.\n\n# Standardize the outcome and predictor\nkeith = keith |&gt;\n  mutate(\n    z_gpa = (gpa - mean(gpa)) / sd(gpa),\n    z_homework = (homework - mean(homework)) / sd(homework),\n  )\n\n# View updated data\nhead(keith)\n\n\n\n  \n\n\n# Fit standardized regression\nlm.z = lm(z_gpa ~ 1 + z_homework, data = keith)\nlm.z\n\n\nCall:\nlm(formula = z_gpa ~ 1 + z_homework, data = keith)\n\nCoefficients:\n(Intercept)   z_homework  \n  7.627e-17    3.274e-01  \n\n\nThe fitted regression equation is:\n\\[\n\\hat{z}_{\\mathrm{GPA}_i} = 0 + 0.327(z_{\\mathrm{Homework}_i})\n\\]\nThe intercept in a standardized regression is always 0.4 Notice that the slope of the standardized regression is the correlation between the unstandardized variables. If we interpret these coefficients:\n\nThe predicted mean standardized GPA for all students who have a standardized value of homework of 0 is 0.\nEach one-unit difference in the standardized value of homework is associated with a 0.327-unit difference in predicted standardized GPA.\n\nRemember that standardized variables have a mean equal to 0 and a standard deviation equal to 1. Using that, these interpretations can be revised to:\n\nThe predicted mean GPA for all students who spend the mean amount of time on homework is the mean GPA.\nEach one-standard deviation difference in time spent on homework is associated with a 0.327-standard deviation difference in predicted GPA.\n\nHere is a scatterplot of the standardized variables along with the fitted standardized regression line. This will help you visually see the results of the standardized regression analysis.\n\nggplot(data = keith, aes(x = z_homework, y = z_gpa)) +\n  geom_point() +\n  theme_bw() +\n  xlab(\"Time spent on homework (standardized)\") +\n  ylab(\"GPA (standardized)\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_abline(intercept = 0, slope = 0.327)\n\n\n\n\nPlot of the standardized GPA versus the standardized time spent on homework for the 100 observations. The mean values are also displayed (dashed lines) along with the fitted regression line (solid line).\n\n\n\n\nUsing standardized regression results allows us to talk about the effect of \\(x\\) on \\(y\\) in a standard metric (standard deviation difference). This can be quite helpful when the unstandardized metric is less meaningful. This is also why some researchers refer to correlation as an effect, even though the value of \\(R^2\\) is more useful in summarizing the usefulness of the model. Standardized regression also makes the intercept interpretable, since the mean value of \\(x\\) is not extrapolated.\n\n\n8.5.1 A Slick Property of the Regression Line\nNotice from the previous scatterplot of the standardized regression results that the standardized regression line goes through the point \\((0,0)\\). Since the variables are standardized, this is the point \\((\\bar{x}, \\bar{y})\\). The regression line will always go through the point \\((\\bar{x}, \\bar{y})\\) even if the variables are unstandardized. This is an important property of the regression line.\nWe can show this property mathematically by predicting \\(y\\) when \\(x\\) is at its mean. The predicted value when \\(x=\\bar{x}\\) is then\n\\[\n\\hat{Y}_i = \\hat\\beta_0 + \\hat\\beta_1(\\bar{x})\n\\]\nUsing a common formula for the regression intercept,\n\\[\n\\hat\\beta_0 = \\bar{y} - \\hat\\beta_1(\\bar{x}),\n\\]\nand substituting this into the prediction equation:\n\\[\n\\begin{split}\n\\hat{Y}_i &= \\hat\\beta_0 + \\hat\\beta_1(\\bar{x}) \\\\\n&= \\bar{y} - \\hat\\beta_1(\\bar{x}) + \\hat\\beta_1(\\bar{x}) \\\\\n&= \\bar{y}\n\\end{split}\n\\]\nThis implies that \\((\\bar{x}, \\bar{y})\\) is always on the regression line and that the predicted value of \\(y\\) for \\(x\\)-values at the mean is always the mean of \\(y\\).\n\n\n\n8.5.2 Variance Accounted For in a Standardized Regression\nThe \\(R^2\\) value for the standardized and unstandardized regression models are identical. That is because the correlation between \\(x\\) and \\(y\\) and that between \\(z_x\\) and \\(z_y\\) are identical (see below). Thus the squared correlation will also be the same, in this case \\(R^2 = 0.327^2 = 0.107\\).\n\nkeith |&gt;\n  select(z_gpa, z_homework) |&gt;\n  correlate()\n\n\n\n  \n\n\n\nWe can also compute \\(R^2\\) as the proportion reduction in error variation (PRE) from the intercept-only model. To do so we again compute the sum of squared error (SSE) for the standardized models (intercept-only and intercept-slope) and determine how much variation was explained by including the standardized amount of time spent on homework as a predictor.\nRemember that the intercept-only model is referred to as the marginal mean model‚Äîit predicts the marginal mean of \\(y\\) regardless of the value of \\(x\\). Since the variables are standardized, the marginal mean of \\(y\\) is 0. Thus the equation for the intercept-only model when the variables are standardized is:\n\\[\n\\hat{z}_{\\mathrm{GPA}} = 0\n\\]\nYou could also fit the intercept-only model to obtain this result, lm(z_gpa ~ 1, data = keith). We can now compute the SSE based on the intercept-only model.\n\n# Compute the SSE for the standardized intercept-only model\nkeith |&gt;\n  mutate(\n    y_hat = 0,\n    errors = z_gpa - y_hat,\n    sq_errors = errors ^ 2\n  ) |&gt;\n  summarize(\n    SSE = sum(sq_errors)\n  )\n\n\n\n  \n\n\n\nWe also compute the SSE for the standardized model that includes the standardized predictor of time spent on homework.\n\n# Compute the SSE for the standardized slope-intercept model\nkeith |&gt;\n  mutate(\n    y_hat = 0 + 0.327 * z_homework,\n    errors = z_gpa - y_hat,\n    sq_errors = errors ^ 2\n  ) |&gt;\n  summarize(\n    SSE = sum(sq_errors)\n  )\n\n\n\n  \n\n\n\nThe proportion reduction in SSE is:\n\\[\nR^2 = \\frac{99 - 88.39}{99} = 0.107\n\\]\nWe can say that differences in time spent on homework explains 10.7% of the variation in GPAs, and that 89.3% of the varition in GPAs remains unexplained. Note that if we compute the SSEs for the unstandardized models, they will be different than the SSEs for the standardized models (after all, they are in a different metric), but they will be in the same proportion, which produces the same \\(R^2\\) value."
  },
  {
    "objectID": "02-03-correlation.html#correlation-between-observed-values-fitted-values-and-residuals",
    "href": "02-03-correlation.html#correlation-between-observed-values-fitted-values-and-residuals",
    "title": "8¬† Correlation and Standardized Regression",
    "section": "8.6 Correlation Between Observed Values, Fitted Values, and Residuals",
    "text": "8.6 Correlation Between Observed Values, Fitted Values, and Residuals\nHere we examine a correlation matrix displaying the correlations between:\n\nThe observed values (\\(y_i\\)) and the fitted values (\\(\\hat{y}_i\\)),\nThe observed values (\\(y_i\\)) and the residuals (\\(e_i\\)), and\nThe fitted values and the residuals.\n\nIt doesn‚Äôt matter whether you use the unstandardized or standardized regression model here, but to illustrate, we will use the unstandardized model.\n\nkeith |&gt;\n  mutate(\n    y_hat = 74.290 + 1.214 * homework,\n    errors = gpa - y_hat\n  ) |&gt;\n  select(gpa, y_hat, errors) |&gt;\n  correlate()\n\n\n\n  \n\n\n\nThe first correlation between the observed values and the fitted values is 0.327. This is the same as the correlation between x and y. This is because the fitted values are just a linear transformation of x. In other words, the fitted values have the same relationship with y as x has with y. Note that if we square this value we get the \\(R^2\\) value for the model. So another way of computing \\(R^2\\) is to square the correlation between y and \\(\\hat{y}\\).\n\\[\nR^2 = (r_{y,\\hat{y}})^2\n\\]\nThe second correlation between the observed values and the residuals is 0.945. This is the value you get if you take the unexplained amount of variation from the model (0.893) and take its square root. Thus it gives us an indication of the unexplained variation in the model.\n\\[\n1 - R^2 = (r_{y,e})^2\n\\]\nThe last correlation between the fitted values and the residuals is 0. That is because the regression model assumes that the errors are independent of the fitted values. We have pulled out all of the information related to x out of the observed y-values (the fitted values) and what is left over is completely unrelated to x (the residuals). When a correlation is 0, statisticians say they two variables are independent of one another. Thus the fitted values and the residuals are said to be independent of one another.\n\\[\nr_{\\hat{y},e} = 0\n\\]"
  },
  {
    "objectID": "02-03-correlation.html#footnotes",
    "href": "02-03-correlation.html#footnotes",
    "title": "8¬† Correlation and Standardized Regression",
    "section": "",
    "text": "The correlation coefficient between observed outcome values and model predicted values uses an upper-case \\(R\\) rather than the lower-case r.‚Ü©Ô∏é\nTechnically divide by the total degrees of freedom, but for large values of n this difference is minor.‚Ü©Ô∏é\nThe sum also depends on the magnitude of the products. For example, if the magnitude of each the negative products is much higher than that for each of the positive products, the sum will be negative despite more positive products.‚Ü©Ô∏é\nR or other statistical software might round this to a very small number. The intercept should always be reported as zero, or dropped from the fitted equation.‚Ü©Ô∏é"
  }
]